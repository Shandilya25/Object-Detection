{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a87585-08f1-470e-8d58-a1c0f119dbce",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45778f-ebf5-4fef-9dcf-4dafbeac2216",
   "metadata": {},
   "source": [
    "## FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987c74b8-1135-4fdd-9029-51a5452b245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "PyTorch Version: 2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n",
    "print(\"PyTorch Version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b40a82-7943-4400-b6ae-03d45e0382dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274d441e-8261-40b9-baa9-5adc42f1d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2d98ef-cddc-4d03-8cfb-5a57b03e40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_list import ImageList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270efc76-1767-42a2-958d-f63acfce185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(FPN, self).__init__()\n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # Bottom-up layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # Top layer\n",
    "        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n",
    "\n",
    "        # Smooth layers\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Lateral layers\n",
    "        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != block.expansion * planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, block.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(block.expansion * planes)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        _, _, H, W = y.size()\n",
    "        return F.upsample(x, size=(H, W), mode='bilinear') + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom-up\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        c1 = self.maxpool(x)\n",
    "\n",
    "        c2 = self.layer1(c1)\n",
    "        c3 = self.layer2(c2)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "        # Top-down\n",
    "        p5 = self.toplayer(c5)\n",
    "        p4 = self._upsample_add(p5, self.latlayer1(c4))\n",
    "        p3 = self._upsample_add(p4, self.latlayer2(c3))\n",
    "        p2 = self._upsample_add(p3, self.latlayer3(c2))\n",
    "        # Smooth\n",
    "        p4 = self.smooth1(p4)\n",
    "        p3 = self.smooth2(p3)\n",
    "        p2 = self.smooth3(p2)\n",
    "        return p2, p3, p4, p5\n",
    "\n",
    "\n",
    "def FPN101():\n",
    "    return FPN(Bottleneck, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8f26f-5e6f-4e99-a826-27d90e1add40",
   "metadata": {},
   "source": [
    "### Hrnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2274f176-5d1c-4f17-9493-87782a3e894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, bn_momentum=0.1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=bn_momentum)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, bn_momentum=0.1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self, stage, output_branches, c, bn_momentum):\n",
    "        super(StageModule, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.output_branches = output_branches\n",
    "\n",
    "        self.branches = nn.ModuleList()\n",
    "        for i in range(self.stage):\n",
    "            w = c * (2 ** i)\n",
    "            branch = nn.Sequential(\n",
    "                BasicBlock(w, w, bn_momentum=bn_momentum),\n",
    "                BasicBlock(w, w, bn_momentum=bn_momentum),\n",
    "                BasicBlock(w, w, bn_momentum=bn_momentum),\n",
    "                BasicBlock(w, w, bn_momentum=bn_momentum),\n",
    "            )\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        self.fuse_layers = nn.ModuleList()\n",
    "        # for each output_branches (i.e. each branch in all cases but the very last one)\n",
    "        for i in range(self.output_branches):\n",
    "            self.fuse_layers.append(nn.ModuleList())\n",
    "            for j in range(self.stage):  # for each branch\n",
    "                if i == j:\n",
    "                    self.fuse_layers[-1].append(nn.Sequential())  # Used in place of \"None\" because it is callable\n",
    "                elif i < j:\n",
    "                    self.fuse_layers[-1].append(nn.Sequential(\n",
    "                        nn.Conv2d(c * (2 ** j), c * (2 ** i), kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "                        nn.BatchNorm2d(c * (2 ** i), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                        nn.Upsample(scale_factor=(2.0 ** (j - i)), mode='nearest'),\n",
    "                    ))\n",
    "                elif i > j:\n",
    "                    ops = []\n",
    "                    for k in range(i - j - 1):\n",
    "                        ops.append(nn.Sequential(\n",
    "                            nn.Conv2d(c * (2 ** j), c * (2 ** j), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                      bias=False),\n",
    "                            nn.BatchNorm2d(c * (2 ** j), eps=1e-05, momentum=0.1, affine=True,\n",
    "                                           track_running_stats=True),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                        ))\n",
    "                    ops.append(nn.Sequential(\n",
    "                        nn.Conv2d(c * (2 ** j), c * (2 ** i), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                  bias=False),\n",
    "                        nn.BatchNorm2d(c * (2 ** i), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                    ))\n",
    "                    self.fuse_layers[-1].append(nn.Sequential(*ops))\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(self.branches) == len(x)\n",
    "\n",
    "        x = [branch(b) for branch, b in zip(self.branches, x)]\n",
    "\n",
    "        x_fused = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            for j in range(0, len(self.branches)):\n",
    "                if j == 0:\n",
    "                    x_fused.append(self.fuse_layers[i][0](x[0]))\n",
    "                else:\n",
    "                    x_fused[i] = x_fused[i] + self.fuse_layers[i][j](x[j])\n",
    "\n",
    "        for i in range(len(x_fused)):\n",
    "            x_fused[i] = self.relu(x_fused[i])\n",
    "\n",
    "        return x_fused\n",
    "\n",
    "\n",
    "class HRNet(nn.Module):\n",
    "    def __init__(self, c=48, nof_joints=17, bn_momentum=0.1):\n",
    "        super(HRNet, self).__init__()\n",
    "\n",
    "        # Input (stem net)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Stage 1 (layer1)      - First group of bottleneck (resnet) modules\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            Bottleneck(64, 64, downsample=downsample),\n",
    "            Bottleneck(256, 64),\n",
    "            Bottleneck(256, 64),\n",
    "            Bottleneck(256, 64),\n",
    "        )\n",
    "\n",
    "        # Fusion layer 1 (transition1)      - Creation of the first two branches (one full and one half resolution)\n",
    "        self.transition1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, c, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(c, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            nn.Sequential(nn.Sequential(  # Double Sequential to fit with official pretrained weights\n",
    "                nn.Conv2d(256, c * (2 ** 1), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(c * (2 ** 1), eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Stage 2 (stage2)      - Second module with 1 group of bottleneck (resnet) modules. This has 2 branches\n",
    "        self.stage2 = nn.Sequential(\n",
    "            StageModule(stage=2, output_branches=2, c=c, bn_momentum=bn_momentum),\n",
    "        )\n",
    "\n",
    "        # Fusion layer 2 (transition2)      - Creation of the third branch (1/4 resolution)\n",
    "        self.transition2 = nn.ModuleList([\n",
    "            nn.Sequential(),  # None,   - Used in place of \"None\" because it is callable\n",
    "            nn.Sequential(),  # None,   - Used in place of \"None\" because it is callable\n",
    "            nn.Sequential(nn.Sequential(  # Double Sequential to fit with official pretrained weights\n",
    "                nn.Conv2d(c * (2 ** 1), c * (2 ** 2), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(c * (2 ** 2), eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )),  # ToDo Why the new branch derives from the \"upper\" branch only?\n",
    "        ])\n",
    "\n",
    "        # Stage 3 (stage3)      - Third module with 4 groups of bottleneck (resnet) modules. This has 3 branches\n",
    "        self.stage3 = nn.Sequential(\n",
    "            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n",
    "            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n",
    "            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n",
    "            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n",
    "        )\n",
    "\n",
    "        # Fusion layer 3 (transition3)      - Creation of the fourth branch (1/8 resolution)\n",
    "        self.transition3 = nn.ModuleList([\n",
    "            nn.Sequential(),  # None,   - Used in place of \"None\" because it is callable\n",
    "            nn.Sequential(),  # None,   - Used in place of \"None\" because it is callable\n",
    "            nn.Sequential(),  # None,   - Used in place of \"None\" because it is callable\n",
    "            nn.Sequential(nn.Sequential(  # Double Sequential to fit with official pretrained weights\n",
    "                nn.Conv2d(c * (2 ** 2), c * (2 ** 3), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(c * (2 ** 3), eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Stage 4 (stage4)      - Fourth module with 3 groups of bottleneck (resnet) modules. This has 4 branches\n",
    "        self.stage4 = nn.Sequential(\n",
    "            StageModule(stage=4, output_branches=4, c=c, bn_momentum=bn_momentum),\n",
    "            StageModule(stage=4, output_branches=4, c=c, bn_momentum=bn_momentum),\n",
    "            StageModule(stage=4, output_branches=1, c=c, bn_momentum=bn_momentum),\n",
    "        )\n",
    "\n",
    "        # Final layer (final_layer)\n",
    "        self.final_layer = nn.Conv2d(c, nof_joints, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = [trans(x) for trans in self.transition1]  # Since now, x is a list (# == nof branches)\n",
    "\n",
    "        x = self.stage2(x)\n",
    "        # x = [trans(x[-1]) for trans in self.transition2]    # New branch derives from the \"upper\" branch only\n",
    "        x = [\n",
    "            self.transition2[0](x[0]),\n",
    "            self.transition2[1](x[1]),\n",
    "            self.transition2[2](x[-1])\n",
    "        ]  # New branch derives from the \"upper\" branch only\n",
    "\n",
    "        x = self.stage3(x)\n",
    "        # x = [trans(x) for trans in self.transition3]    # New branch derives from the \"upper\" branch only\n",
    "        x = [\n",
    "            self.transition3[0](x[0]),\n",
    "            self.transition3[1](x[1]),\n",
    "            self.transition3[2](x[2]),\n",
    "            self.transition3[3](x[-1])\n",
    "        ]  # New branch derives from the \"upper\" branch only\n",
    "\n",
    "        x = self.stage4(x)\n",
    "\n",
    "        x = self.final_layer(x[0])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bc422-8e49-496b-a8a7-dae82a4be1a3",
   "metadata": {},
   "source": [
    "### Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bb24c6-109e-474a-a01e-0b476eef0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def _make_divisible(ch, divisor=8, min_ch=None):\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, groups=1, norm_layer=None):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            norm_layer(out_channel),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride, expand_ratio, norm_layer=None):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        hidden_channel = in_channel * expand_ratio\n",
    "        self.use_shortcut = stride == 1 and in_channel == out_channel\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # 1x1 pointwise conv\n",
    "            layers.append(ConvBNReLU(in_channel, hidden_channel, kernel_size=1, norm_layer=norm_layer))\n",
    "        layers.extend([\n",
    "            # 3x3 depthwise conv\n",
    "            ConvBNReLU(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel, norm_layer=norm_layer),\n",
    "            # 1x1 pointwise conv(linear)\n",
    "            nn.Conv2d(hidden_channel, out_channel, kernel_size=1, bias=False),\n",
    "            norm_layer(out_channel),\n",
    "        ])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_shortcut:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, alpha=1.0, round_nearest=8, weights_path=None, norm_layer=None):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = _make_divisible(32 * alpha, round_nearest)\n",
    "        last_channel = _make_divisible(1280 * alpha, round_nearest)\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        inverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        features = []\n",
    "        # conv1 layer\n",
    "        features.append(ConvBNReLU(3, input_channel, stride=2, norm_layer=norm_layer))\n",
    "        # building inverted residual residual blockes\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * alpha, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, last_channel, 1, norm_layer=norm_layer))\n",
    "        # combine feature layers\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(last_channel, num_classes)\n",
    "        )\n",
    "\n",
    "        if weights_path is None:\n",
    "            # weight initialization\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc988c-ac7d-4006-9bd1-24801c99ce97",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0904322c-a0c6-488a-aada-73b048139f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import Tuple, List, Dict\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = norm_layer(out_channel)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = norm_layer(out_channel)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion,\n",
    "                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = norm_layer(out_channel * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, blocks_num, num_classes=1000, include_top=True, norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                norm_layer(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel, channel, downsample=downsample,\n",
    "                            stride=stride, norm_layer=norm_layer))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel, channel, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "    Arguments:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "    \"\"\"\n",
    "    __annotations__ = {\n",
    "        \"return_layers\": Dict[str, str],\n",
    "    }\n",
    "\n",
    "    def __init__(self, model, return_layers):\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                out[out_name] = x\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeaturePyramidNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that adds a FPN from on top of a set of feature maps. This is based on\n",
    "    `\"Feature Pyramid Network for Object Detection\" <https://arxiv.org/abs/1612.03144>`_.\n",
    "    The feature maps are currently supposed to be in increasing depth\n",
    "    order.\n",
    "    The input to the model is expected to be an OrderedDict[Tensor], containing\n",
    "    the feature maps on top of which the FPN will be added.\n",
    "    Arguments:\n",
    "        in_channels_list (list[int]): number of channels for each feature map that\n",
    "            is passed to the module\n",
    "        out_channels (int): number of channels of the FPN representation\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels_list, out_channels, extra_blocks=None):\n",
    "        super(FeaturePyramidNetwork, self).__init__()\n",
    "        self.inner_blocks = nn.ModuleList()\n",
    "        self.layer_blocks = nn.ModuleList()\n",
    "        for in_channels in in_channels_list:\n",
    "            if in_channels == 0:\n",
    "                continue\n",
    "            inner_block_module = nn.Conv2d(in_channels, out_channels, 1)\n",
    "            layer_block_module = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "            self.inner_blocks.append(inner_block_module)\n",
    "            self.layer_blocks.append(layer_block_module)\n",
    "\n",
    "        # initialize parameters now to avoid modifying the initialization of top_blocks\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.extra_blocks = extra_blocks\n",
    "\n",
    "    def get_result_from_inner_blocks(self, x, idx):\n",
    "        # type: (Tensor, int) -> Tensor\n",
    "        \"\"\"\n",
    "        This is equivalent to self.inner_blocks[idx](x),\n",
    "        but torchscript doesn't support this yet\n",
    "        \"\"\"\n",
    "        num_blocks = 0\n",
    "        for m in self.inner_blocks:\n",
    "            num_blocks += 1\n",
    "        if idx < 0:\n",
    "            idx += num_blocks\n",
    "        i = 0\n",
    "        out = x\n",
    "        for module in self.inner_blocks:\n",
    "            if i == idx:\n",
    "                out = module(x)\n",
    "            i += 1\n",
    "        return out\n",
    "\n",
    "    def get_result_from_layer_blocks(self, x, idx):\n",
    "        # type: (Tensor, int) -> Tensor\n",
    "        \"\"\"\n",
    "        This is equivalent to self.layer_blocks[idx](x),\n",
    "        but torchscript doesn't support this yet\n",
    "        \"\"\"\n",
    "        num_blocks = 0\n",
    "        for m in self.layer_blocks:\n",
    "            num_blocks += 1\n",
    "        if idx < 0:\n",
    "            idx += num_blocks\n",
    "        i = 0\n",
    "        out = x\n",
    "        for module in self.layer_blocks:\n",
    "            if i == idx:\n",
    "                out = module(x)\n",
    "            i += 1\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Dict[str, Tensor]) -> Dict[str, Tensor]\n",
    "        \"\"\"\n",
    "        Computes the FPN for a set of feature maps.\n",
    "        Arguments:\n",
    "            x (OrderedDict[Tensor]): feature maps for each feature level.\n",
    "        Returns:\n",
    "            results (OrderedDict[Tensor]): feature maps after FPN layers.\n",
    "                They are ordered from highest resolution first.\n",
    "        \"\"\"\n",
    "        names = list(x.keys())\n",
    "        x = list(x.values())\n",
    "\n",
    "        last_inner = self.get_result_from_inner_blocks(x[-1], -1)\n",
    "\n",
    "        results = []\n",
    "        results.append(self.get_result_from_layer_blocks(last_inner, -1))\n",
    "\n",
    "        for idx in range(len(x) - 2, -1, -1):\n",
    "            inner_lateral = self.get_result_from_inner_blocks(x[idx], idx)\n",
    "            feat_shape = inner_lateral.shape[-2:]\n",
    "            inner_top_down = F.interpolate(last_inner, size=feat_shape, mode=\"nearest\")\n",
    "            last_inner = inner_lateral + inner_top_down\n",
    "            results.insert(0, self.get_result_from_layer_blocks(last_inner, idx))\n",
    "\n",
    "        if self.extra_blocks is not None:\n",
    "            results, names = self.extra_blocks(results, names)\n",
    "\n",
    "        # make it back an OrderedDict\n",
    "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LastLevelMaxPool(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a max_pool2d on top of the last feature map\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, names):\n",
    "        names.append(\"pool\")\n",
    "        x.append(F.max_pool2d(x[-1], 1, 2, 0))\n",
    "        return x, names\n",
    "\n",
    "\n",
    "class BackboneWithFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds a FPN on top of a model.\n",
    "    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to\n",
    "    extract a submodel that returns the feature maps specified in return_layers.\n",
    "    The same limitations of IntermediatLayerGetter apply here.\n",
    "    Arguments:\n",
    "        backbone (nn.Module)\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "        in_channels_list (List[int]): number of channels for each feature map\n",
    "            that is returned, in the order they are present in the OrderedDict\n",
    "        out_channels (int): number of channels in the FPN.\n",
    "    Attributes:\n",
    "        out_channels (int): the number of channels in the FPN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, return_layers, in_channels_list, out_channels):\n",
    "        super(BackboneWithFPN, self).__init__()\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "            extra_blocks=LastLevelMaxPool(),\n",
    "        )\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.fpn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet50_fpn_backbone():\n",
    "\n",
    "    resnet_backbone = ResNet(Bottleneck, [3, 4, 6, 3],\n",
    "                             include_top=False)\n",
    "\n",
    "    for name, parameter in resnet_backbone.named_parameters():\n",
    "        if 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "    return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}\n",
    "\n",
    "    in_channels_stage2 = resnet_backbone.in_channel // 8\n",
    "    in_channels_list = [\n",
    "        in_channels_stage2,  # layer1 out_channel=256\n",
    "        in_channels_stage2 * 2,  # layer2 out_channel=512\n",
    "        in_channels_stage2 * 4,  # layer3 out_channel=1024\n",
    "        in_channels_stage2 * 8,  # layer4 out_channel=2048\n",
    "    ]\n",
    "    out_channels = 256\n",
    "    return BackboneWithFPN(resnet_backbone, return_layers, in_channels_list, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbef950-b0a9-493d-8a59-cdcbbdbad7d2",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecee46cf-b189-4469-b8da-6ee93f1ed8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "channels_cfgs = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "\n",
    "def vgg16(weights_path=None):\n",
    "    model = VGG(make_features(channels_cfgs), weights_path=weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, class_num=1000, init_weights=False, weights_path=None):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512 * 7 * 7, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, class_num)\n",
    "        )\n",
    "        if init_weights and weights_path is None:\n",
    "            self._initialize_weights()\n",
    "\n",
    "        if weights_path is not None:\n",
    "            self.load_state_dict(torch.load(weights_path), strict=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.features(x)\n",
    "        # N x 512 x 7 x 7\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # N x 512*7*7\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_features(ch_cfgs):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in ch_cfgs:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU(True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b1a73-673d-4c9e-b2f4-1150bde7e34c",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a45f134-9bf3-4f1a-978d-b9fd5f89dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_weights = \" \"\n",
    "    image_path = \" \"\n",
    "    gpu_id = '2'\n",
    "    num_classes = 80 + 1\n",
    "    data_root_dir = \" \"\n",
    "\n",
    "\n",
    "# test_cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009dd606-1848-4a0f-8838-fed1e1e1f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Config:\n",
    "    backbone = 'resnet50_fpn'  # [vgg16, resnet-fpn, mobilenet, resnet50_fpn]\n",
    "    backbone_pretrained_weights = 'fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'  # [path or None]\n",
    "\n",
    "    # data transform parameter\n",
    "    train_horizon_flip_prob = 0.0  # data horizon flip probility in train transform\n",
    "    min_size = 800\n",
    "    max_size = 1000\n",
    "    image_mean = [0.485, 0.456, 0.406]\n",
    "    image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # anchor parameters\n",
    "    anchor_size = [64, 128, 256]\n",
    "    anchor_ratio = [0.5, 1, 2.0]\n",
    "\n",
    "    # roi align parameters\n",
    "    roi_out_size = [7, 7]\n",
    "    roi_sample_rate = 2\n",
    "\n",
    "    # rpn process parameters\n",
    "    rpn_pre_nms_top_n_train = 2000\n",
    "    rpn_post_nms_top_n_train = 2000\n",
    "\n",
    "    rpn_pre_nms_top_n_test = 1000\n",
    "    rpn_post_nms_top_n_test = 1000\n",
    "\n",
    "    rpn_nms_thresh = 0.7\n",
    "    rpn_fg_iou_thresh = 0.7\n",
    "    rpn_bg_iou_thresh = 0.3\n",
    "    rpn_batch_size_per_image = 256\n",
    "    rpn_positive_fraction = 0.5\n",
    "\n",
    "    # remove low threshold target\n",
    "    box_score_thresh = 0.05\n",
    "    box_nms_thresh = 0.5\n",
    "    box_detections_per_img = 100\n",
    "    box_fg_iou_thresh = 0.5\n",
    "    box_bg_iou_thresh = 0.5\n",
    "    box_batch_size_per_image = 512\n",
    "    box_positive_fraction = 0.25\n",
    "    bbox_reg_weights = None\n",
    "\n",
    "    device_name = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    resume = ''  # pretrained_weights\n",
    "    start_epoch = 0  # start epoch\n",
    "    num_epochs = 5000  # train epochs\n",
    "\n",
    "    # learning rate parameters\n",
    "    lr = 5e-3\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "\n",
    "    # learning rate schedule\n",
    "    lr_gamma = 0.33\n",
    "    lr_dec_step_size = 100\n",
    "\n",
    "    batch_size = 6\n",
    "\n",
    "    num_class = 80 + 1  # foreground + 1 background\n",
    "    data_root_dir = \" \"\n",
    "    model_save_dir = \"Best_model_frcnn\"\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f877e1-8804-43c2-bf80-272086f6cc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a56a953-34eb-43f6-88d3-923d54f0d4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92b768ab-2dca-43d9-8128-ae90db9356c5",
   "metadata": {},
   "source": [
    "## anchor test util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33cbd83a-2f04-4c1e-a64e-0457f4f4bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.anchor_utils import generate_anchors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19aa210-807a-4c7c-a59a-7390d1139a3d",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803b5c8e-a2f0-49be-a88d-d5de3175693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def generate_anchors(scales, aspect_ratios, dtype=torch.float32, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "     generate anchor template based on sizes and ratios, generated template is centered at [0, 0]\n",
    "     :param scales: anchor sizes, in tuple[int]\n",
    "     :param aspect_ratios: anchor ratios, in tuple[float]\n",
    "     :param dtype: data type\n",
    "     :param device: date device\n",
    "     :return:\n",
    "     \"\"\"\n",
    "\n",
    "    scales = torch.as_tensor(scales, dtype=dtype, device=device)\n",
    "    aspect_ratios = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)\n",
    "    h_ratios = torch.sqrt(aspect_ratios)\n",
    "    w_ratios = 1.0 / h_ratios\n",
    "\n",
    "    # [r1, r2, r3]' * [s1, s2, s3]\n",
    "    # number of elements is len(ratios)*len(scales)\n",
    "    ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "    hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "\n",
    "    # left-top, right-bottom coordinate relative to anchor center(0, 0)\n",
    "    # anchor template is centered at [0, 0], shape [len(ratios)*len(scales), 4]\n",
    "    base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "\n",
    "    return base_anchors.round()  # anchor will lose some precision here\n",
    "\n",
    "\n",
    "class AnchorsGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    anchor generator for feature maps according to anchor sizes and ratios\n",
    "    :param sizes: anchor sizes, in tuple[int]\n",
    "    :param aspect_ratios: anchor ratios, in tuple[float]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes=(128, 256, 512), aspect_ratios=(0.5, 1.0, 2.0)):\n",
    "        super(AnchorsGenerator, self).__init__()\n",
    "\n",
    "        # assert len(sizes) == len(aspect_ratios), 'anchor sizes must equal to anchor ratios!'\n",
    "\n",
    "        self.sizes = sizes\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.cell_anchors = None\n",
    "        self._cache = {}\n",
    "\n",
    "    def set_cell_anchors(self, dtype, device):\n",
    "        \"\"\"\n",
    "        generate template template\n",
    "        :param dtype: data type\n",
    "        :param device: data device\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.cell_anchors is not None:\n",
    "            cell_anchors = self.cell_anchors\n",
    "            assert cell_anchors is not None\n",
    "\n",
    "        # generate anchor template\n",
    "        cell_anchors = [generate_anchors(sizes, aspect_ratios, dtype, device)\n",
    "                        for sizes, aspect_ratios in zip(self.sizes, self.aspect_ratios)]\n",
    "        self.cell_anchors = cell_anchors\n",
    "\n",
    "    def num_anchors_per_location(self):\n",
    "        # calculate the number of anchors per feature map, for k in origin paper\n",
    "        return [len(s) * len(a) for s, a in zip(self.sizes, self.aspect_ratios)]\n",
    "\n",
    "    def grid_anchors(self, feature_map_sizes, strides):\n",
    "        \"\"\"\n",
    "        compute anchor coordinate list in origin image, mapped from feature map\n",
    "        :param feature_map_sizes: feature map sizes\n",
    "        :param strides: strides between origin image and anchor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        anchors = []\n",
    "        cell_anchors = self.cell_anchors  # anchor template\n",
    "        assert cell_anchors is not None\n",
    "\n",
    "        # for every resolution feature map, like fpn\n",
    "        for size, stride, base_anchors in zip(feature_map_sizes, strides, cell_anchors):\n",
    "            f_p_height, f_p_width = size\n",
    "            stride_height, stride_width = stride\n",
    "            device = base_anchors.device\n",
    "\n",
    "            # For output anchor, compute [x_center, y_center, x_center, y_center...]\n",
    "            # x_center in origin image\n",
    "            shifts_x = torch.arange(0, f_p_width, dtype=torch.float32, device=device) * stride_width\n",
    "\n",
    "            # y_center in origin image\n",
    "            shifts_y = torch.arange(0, f_p_height, dtype=torch.float32, device=device) * stride_height\n",
    "\n",
    "            # torch.meshgrid will output grid\n",
    "            # shape: [grid_height, grid_width]\n",
    "            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n",
    "            shift_x = shift_x.reshape(-1)\n",
    "            shift_y = shift_y.reshape(-1)\n",
    "\n",
    "            shifts = torch.stack([shift_x, shift_y, shift_x, shift_y], dim=1)\n",
    "\n",
    "            # For every (base anchor, output anchor) pair,\n",
    "            # offset each zero-centered base anchor by the center of the output anchor\n",
    "            shifts_anchor = shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)\n",
    "            anchors.append(shifts_anchor.reshape(-1, 4))\n",
    "\n",
    "        return anchors  # List[Tensor(all_num_anchors, 4)]\n",
    "\n",
    "    def cached_grid_anchors(self, feature_map_size, strides):\n",
    "        \"\"\"\n",
    "        cached all anchor information\n",
    "        :param feature_map_size: feature map size after backbone feature extractor\n",
    "        :param strides: strides between origin image size and feature map size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        key = str(feature_map_size) + str(strides)\n",
    "        # self._cache is a dictionary type\n",
    "        if key in self._cache:\n",
    "            return self._cache[key]\n",
    "        anchors = self.grid_anchors(feature_map_size, strides)\n",
    "        self._cache[key] = anchors\n",
    "        return anchors\n",
    "\n",
    "    def forward(self, image_list, feature_maps):\n",
    "        \"\"\"\n",
    "        get feature map sizes\n",
    "        :param image_list:\n",
    "        :param feature_maps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        feature_map_sizes = list([feature_map.shape[-2:] for feature_map in feature_maps])\n",
    "\n",
    "        # get input image sizes\n",
    "        image_size = image_list.tensors.shape[-2:]\n",
    "\n",
    "        # get dtype and device\n",
    "        dtype, device = feature_maps[0].dtype, feature_maps[0].device\n",
    "\n",
    "        # compute map stride between feature_maps and input images\n",
    "        # strides = [[torch.tensor(image_size[0] / g[0], dtype=torch.int64, device=device),\n",
    "        #             torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in feature_map_sizes]\n",
    "        strides = [[image_size[0] // g[0], image_size[1] // g[1]] for g in feature_map_sizes]\n",
    "\n",
    "\n",
    "        # get anchors template according size and aspect_ratios\n",
    "        self.set_cell_anchors(dtype, device)\n",
    "\n",
    "        # get anchor coordinate list in origin image, according to map\n",
    "        anchors_over_all_feature_maps = self.cached_grid_anchors(feature_map_sizes, strides)\n",
    "\n",
    "        anchors = []\n",
    "        # for every image and feature map in a batch\n",
    "        for i, (_, _) in enumerate(image_list.image_sizes):\n",
    "            anchors_in_image = []\n",
    "            # for every resolution feature map like fpn\n",
    "            for anchors_per_feature_map in anchors_over_all_feature_maps:\n",
    "                anchors_in_image.append(anchors_per_feature_map)\n",
    "            anchors.append(anchors_in_image)\n",
    "\n",
    "        # concat every resolution anchors, like fpn\n",
    "        anchors = [torch.cat(anchors_per_image) for anchors_per_image in anchors]\n",
    "\n",
    "        self._cache.clear()\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82727fc2-8112-45fe-9a86-2d1565bb2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def nms(boxes, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "     Performs non-maximum suppression (NMS) on the boxes according to their intersection-over-union (IoU).\n",
    "\n",
    "    NMS iteratively removes lower scoring boxes which have an IoU greater than iou_threshold with another (higher scoring)\n",
    "    box.\n",
    "    :param boxes: Tensor[N, 4]), boxes to perform NMS on. They are expected to be in (x1, y1, x2, y2) format\n",
    "    :param scores: Tensor[N], scores for each one of the boxes\n",
    "    :param iou_threshold: float, discards all overlapping boxes with IoU < iou_threshold\n",
    "    :return: int64 tensor with the indices of the elements that have been kept by NMS, sorted in decreasing order of scores\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)\n",
    "\n",
    "\n",
    "def batched_nms(boxes, scores, idxs, iou_threshold):\n",
    "    \"\"\"\n",
    "    Performs non-maximum suppression in a batched fashion.\n",
    "    Each index value correspond to a category, and NMS\n",
    "    will not be applied between elements of different categories\n",
    "    :param boxes: Tensor[N, 4], boxes where NMS will be performed. They are expected to be in (x1, y1, x2, y2) format\n",
    "    :param scores:  Tensor[N], scores for each one of the boxes\n",
    "    :param idxs: Tensor[N], indices of the categories for each one of the boxes.\n",
    "    :param iou_threshold: float, discards all overlapping boxes, with IoU < iou_threshold\n",
    "    :return: int64 tensor with the indices of the elements that have been kept by NMS, sorted\n",
    "        in decreasing order of scores\n",
    "    \"\"\"\n",
    "\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "    # strategy: in order to perform NMS independently per class.\n",
    "    # we add an offset to all the boxes. The offset is dependent\n",
    "    # only on the class idx, and is large enough so that boxes\n",
    "    # from different classes do not overlap\n",
    "    max_coordinate = boxes.max()\n",
    "\n",
    "    # to(): Performs Tensor dtype and/or device conversion\n",
    "    offsets = idxs.to(boxes) * (max_coordinate + 1)\n",
    "    boxes_for_nms = boxes + offsets[:, None]\n",
    "    keep = nms(boxes_for_nms, scores, iou_threshold)\n",
    "    return keep\n",
    "\n",
    "\n",
    "def remove_small_boxes(boxes, min_size):\n",
    "    \"\"\"\n",
    "    Remove boxes which contains at least one side smaller than min_size.\n",
    "    :param boxes: boxes in (x1, y1, x2, y2) format\n",
    "    :param min_size: minimum size\n",
    "    :return: indices of the boxes that have both sides\n",
    "            larger than min_size\n",
    "    \"\"\"\n",
    "\n",
    "    ws, hs = boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1]\n",
    "    keep = (ws >= min_size) & (hs >= min_size)\n",
    "    # nonzero(): Returns a tensor containing the indices of all non-zero elements of input\n",
    "    keep = keep.nonzero().squeeze(1)\n",
    "    return keep\n",
    "\n",
    "\n",
    "def clip_boxes_to_image(boxes, size):\n",
    "    \"\"\"\n",
    "    Clip boxes so that they lie inside an image of size `size`.\n",
    "    :param boxes: boxes in (x1, y1, x2, y2) format\n",
    "    :param size: size of the image\n",
    "    :return: clipped_boxes (Tensor[N, 4])\n",
    "    \"\"\"\n",
    "\n",
    "    dim = boxes.dim()\n",
    "    boxes_x = boxes[..., 0::2]  # x1, x2\n",
    "    boxes_y = boxes[..., 1::2]  # y1, y2\n",
    "    height, width = size\n",
    "\n",
    "    boxes_x = boxes_x.clamp(min=0, max=width)\n",
    "    boxes_y = boxes_y.clamp(min=0, max=height)\n",
    "\n",
    "    clipped_boxes = torch.stack((boxes_x, boxes_y), dim=dim)\n",
    "    return clipped_boxes.reshape(boxes.shape)\n",
    "\n",
    "\n",
    "def box_area(boxes):\n",
    "    \"\"\"\n",
    "    Computes the area of a set of bounding boxes, which are specified by its\n",
    "    (x1, y1, x2, y2) coordinates.\n",
    "    :param boxes:  boxes for which the area will be computed. They\n",
    "                   are expected to be in (x1, y1, x2, y2) format\n",
    "    :return: area for each box\n",
    "    \"\"\"\n",
    "\n",
    "    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "     Calculate intersection-over-union (Jaccard index) of boxes.\n",
    "     Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    :param boxes1: boxes1 (Tensor[N, 4])\n",
    "    :param boxes2: boxes2 (Tensor[M, 4])\n",
    "    :return: iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # left-top [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # right-bottom [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    iou = inter / (area1[:, None] + area2 - inter)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def permute_and_flatten(layer, N, A, C, H, W):\n",
    "    \"\"\"\n",
    "    adjust tensor orderand reshape\n",
    "    :param layer: classification or bboxes parameters\n",
    "    :param N: batch_size\n",
    "    :param A: anchors_num_per_position\n",
    "    :param C: classes_num or bbox coordinate\n",
    "    :param H: height\n",
    "    :param W: width\n",
    "    :return: Tensor after adjusting order and reshaping\n",
    "    \"\"\"\n",
    "\n",
    "    # [batch_size, anchors_num_per_position * (C or 4), height, width]\n",
    "    layer = layer.view(N, -1, C, H, W)\n",
    "    layer = layer.permute(0, 3, 4, 1, 2)  # [N, H, W, -1, C]\n",
    "    layer = layer.reshape(N, -1, C)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def concat_box_prediction_layers(box_cls, box_regression):\n",
    "    \"\"\"\n",
    "    Adjust box classification and bbox regression parameters order and reshape\n",
    "    :param box_cls: target prediction score\n",
    "    :param box_regression: bbox regression parameters\n",
    "    :return: [N, -1, C]\n",
    "    \"\"\"\n",
    "\n",
    "    box_cls_flattened = []\n",
    "    box_regression_flattened = []\n",
    "\n",
    "    for box_cls_per_level, box_regression_per_level in zip(box_cls, box_regression):\n",
    "        # [batch_size, anchors_num_per_position * classes_num, height, width], class_num is equal 2\n",
    "        N, AxC, H, W = box_cls_per_level.shape\n",
    "        # [batch_size, anchors_num_per_position * 4, height, width]\n",
    "        Ax4 = box_regression_per_level.shape[1]\n",
    "        # anchors_num_per_position\n",
    "        A = Ax4 // 4\n",
    "        # classes_num\n",
    "        C = AxC // A\n",
    "\n",
    "        # [N, -1, C]\n",
    "        box_cls_per_level = permute_and_flatten(box_cls_per_level, N, A, C, H, W)\n",
    "        box_cls_flattened.append(box_cls_per_level)\n",
    "\n",
    "        # [N, -1, C]\n",
    "        box_regression_per_level = permute_and_flatten(box_regression_per_level, N, A, 4, H, W)\n",
    "        box_regression_flattened.append(box_regression_per_level)\n",
    "\n",
    "    box_cls = torch.cat(box_cls_flattened, dim=1).flatten(0, -2)  # start_dim, end_dim\n",
    "    box_regression = torch.cat(box_regression_flattened, dim=1).reshape(-1, 4)\n",
    "    return box_cls, box_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c067c-8914-4bf5-9fa8-41aa1c1c538c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b63cdc2-277e-428b-ba69-165c0eccf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class BalancedPositiveNegativeSampler(object):\n",
    "    \"\"\"\n",
    "    This class samples batches, ensuring that they contain a fixed proportion of positives\n",
    "    :param batch_size_per_image: number of elements to be selected per image\n",
    "    :param positive_fraction: percentage of positive elements per batch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size_per_image, positive_fraction):\n",
    "        self.batch_size_per_image = batch_size_per_image\n",
    "        self.positive_fraction = positive_fraction\n",
    "\n",
    "    def __call__(self, matched_idxs):\n",
    "        \"\"\"\n",
    "        Returns two lists of binary masks for each image.\n",
    "        The first list contains the positive elements that were selected,\n",
    "        and the second list the negative example.\n",
    "        :param matched_idxs: list of tensors containing -1, 0 or positive values.\n",
    "                Each tensor corresponds to a specific image.\n",
    "                -1 values are ignored, 0 are considered as negatives and > 0 as\n",
    "                positives.\n",
    "        :return: pos_idx (list[tensor])\n",
    "            neg_idx (list[tensor])\n",
    "        \"\"\"\n",
    "\n",
    "        pos_idx = []\n",
    "        neg_idx = []\n",
    "        for matched_idxs_per_image in matched_idxs:\n",
    "            # positive sample if index >= 1\n",
    "            positive = torch.nonzero(matched_idxs_per_image >= 1).squeeze(1)\n",
    "            # negative sample if index == 0\n",
    "            negative = torch.nonzero(matched_idxs_per_image == 0).squeeze(1)\n",
    "\n",
    "            # number of positive samples\n",
    "            num_pos = int(self.batch_size_per_image * self.positive_fraction)\n",
    "            # protect against not enough positive examples, used all positive samples\n",
    "            num_pos = min(positive.numel(), num_pos)\n",
    "\n",
    "            # number of negative samples\n",
    "            num_neg = self.batch_size_per_image - num_pos\n",
    "            # protect against not enough negative examples, used all negative samples\n",
    "            num_neg = min(negative.numel(), num_neg)\n",
    "\n",
    "            # randomly select positive and negative examples\n",
    "            # Returns a random permutation of integers from 0 to n - 1.\n",
    "            perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n",
    "            perm2 = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n",
    "\n",
    "            pos_idx_per_image = positive[perm1]\n",
    "            neg_idx_per_image = negative[perm2]\n",
    "\n",
    "            # create binary mask from indices\n",
    "            pos_idx_per_image_mask = torch.zeros_like(\n",
    "                matched_idxs_per_image, dtype=torch.uint8\n",
    "            )\n",
    "            neg_idx_per_image_mask = torch.zeros_like(\n",
    "                matched_idxs_per_image, dtype=torch.uint8\n",
    "            )\n",
    "\n",
    "            pos_idx_per_image_mask[pos_idx_per_image] = 1\n",
    "            neg_idx_per_image_mask[neg_idx_per_image] = 1\n",
    "\n",
    "            pos_idx.append(pos_idx_per_image_mask)\n",
    "            neg_idx.append(neg_idx_per_image_mask)\n",
    "\n",
    "        return pos_idx, neg_idx\n",
    "\n",
    "\n",
    "def encode_boxes(reference_boxes, proposals, weights):\n",
    "    \"\"\"\n",
    "    Encode a set of proposals with respect to some reference boxes\n",
    "    :param reference_boxes: reference boxes(gt)\n",
    "    :param proposals: boxes to be encoded(anchors)\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    wx = weights[0]\n",
    "    wy = weights[1]\n",
    "    ww = weights[2]\n",
    "    wh = weights[3]\n",
    "\n",
    "    # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "    proposals_x1 = proposals[:, 0].unsqueeze(1)\n",
    "    proposals_y1 = proposals[:, 1].unsqueeze(1)\n",
    "    proposals_x2 = proposals[:, 2].unsqueeze(1)\n",
    "    proposals_y2 = proposals[:, 3].unsqueeze(1)\n",
    "\n",
    "    reference_boxes_x1 = reference_boxes[:, 0].unsqueeze(1)\n",
    "    reference_boxes_y1 = reference_boxes[:, 1].unsqueeze(1)\n",
    "    reference_boxes_x2 = reference_boxes[:, 2].unsqueeze(1)\n",
    "    reference_boxes_y2 = reference_boxes[:, 3].unsqueeze(1)\n",
    "\n",
    "    # implementation starts here\n",
    "    # parse widths and heights\n",
    "    ex_widths = proposals_x2 - proposals_x1\n",
    "    ex_heights = proposals_y2 - proposals_y1\n",
    "\n",
    "    # center point\n",
    "    ex_ctr_x = proposals_x1 + 0.5 * ex_widths\n",
    "    ex_ctr_y = proposals_y1 + 0.5 * ex_heights\n",
    "\n",
    "    gt_widths = reference_boxes_x2 - reference_boxes_x1\n",
    "    gt_heights = reference_boxes_y2 - reference_boxes_y1\n",
    "    gt_ctr_x = reference_boxes_x1 + 0.5 * gt_widths\n",
    "    gt_ctr_y = reference_boxes_y1 + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = wx * (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = wy * (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = ww * torch.log(gt_widths / ex_widths)\n",
    "    targets_dh = wh * torch.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = torch.cat((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "    return targets\n",
    "\n",
    "\n",
    "class BoxCoder(object):\n",
    "    \"\"\"\n",
    "    This class encodes and decodes a set of bounding boxes into\n",
    "    the representation used for training the regressors.\n",
    "    :param weights: 4-element tuple, represented calculation weights of x, y, h, w\n",
    "    :param bbox_xform_clip: float, represented maximum of height and width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights, bbox_xform_clip=math.log(1000. / 16)):\n",
    "        self.weights = weights\n",
    "        self.bbox_xform_clip = bbox_xform_clip\n",
    "\n",
    "    def encode(self, reference_boxes, proposals):\n",
    "        \"\"\"\n",
    "        This class is inserted to calculate parameters of regression\n",
    "        :param reference_boxes: gt bbox\n",
    "        :param proposals: anchors bbox\n",
    "        :return: regression parameters\n",
    "        \"\"\"\n",
    "\n",
    "        boxes_per_image = [len(b) for b in reference_boxes]\n",
    "        reference_boxes = torch.cat(reference_boxes, dim=0)\n",
    "        proposals = torch.cat(proposals, dim=0)\n",
    "\n",
    "        # targets_dx, targets_dy, targets_dw, targets_dh\n",
    "        targets = self.encode_single(reference_boxes, proposals)\n",
    "        return targets.split(boxes_per_image, 0)\n",
    "\n",
    "    def encode_single(self, reference_boxes, proposals):\n",
    "        \"\"\"\n",
    "        Encode a set of proposals with respect to some reference boxes\n",
    "        :param reference_boxes: reference boxes\n",
    "        :param proposals: boxes to be encoded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        dtype = reference_boxes.dtype\n",
    "        device = reference_boxes.device\n",
    "        weights = torch.as_tensor(self.weights, dtype=dtype, device=device)\n",
    "        targets = encode_boxes(reference_boxes, proposals, weights)\n",
    "\n",
    "        return targets\n",
    "\n",
    "    def decode(self, rel_codes, boxes):\n",
    "        \"\"\"\n",
    "        decode regression parameters\n",
    "        :param rel_codes: bbox regression parameters\n",
    "        :param boxes: anchors\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(boxes, (list, tuple))\n",
    "        assert isinstance(rel_codes, torch.Tensor)\n",
    "\n",
    "        boxes_per_image = [b.size(0) for b in boxes]\n",
    "        concat_boxes = torch.cat(boxes, dim=0)\n",
    "\n",
    "        box_sum = 0\n",
    "        for val in boxes_per_image:\n",
    "            box_sum += val\n",
    "        # map regression parameters into anchors to get coordinate\n",
    "        pred_boxes = self.decode_single(\n",
    "            rel_codes.reshape(box_sum, -1), concat_boxes\n",
    "        )\n",
    "        return pred_boxes.reshape(box_sum, -1, 4)\n",
    "\n",
    "    def decode_single(self, rel_codes, boxes):\n",
    "        \"\"\"\n",
    "        From a set of original boxes and encoded relative box offsets, get the decoded boxes.\n",
    "        :param rel_codes: encoded boxes (bbox regression parameters)\n",
    "        :param boxes: reference boxes (anchors)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        boxes = boxes.to(rel_codes.dtype)\n",
    "\n",
    "        # xmin, ymin, xmax, ymax\n",
    "        widths = boxes[:, 2] - boxes[:, 0]   # anchor width\n",
    "        heights = boxes[:, 3] - boxes[:, 1]  # anchor height\n",
    "        ctr_x = boxes[:, 0] + 0.5 * widths   # anchor center x coordinate\n",
    "        ctr_y = boxes[:, 1] + 0.5 * heights  # anchor center y coordinate\n",
    "\n",
    "        wx, wy, ww, wh = self.weights  # default is 1\n",
    "        dx = rel_codes[:, 0::4] / wx   # predicated anchors center x regression parameters\n",
    "        dy = rel_codes[:, 1::4] / wy   # predicated anchors center y regression parameters\n",
    "        dw = rel_codes[:, 2::4] / ww   # predicated anchors width regression parameters\n",
    "        dh = rel_codes[:, 3::4] / wh   # predicated anchors height regression parameters\n",
    "\n",
    "        # limit max value, prevent sending too large values into torch.exp()\n",
    "        # self.bbox_xform_clip=math.log(1000. / 16)\n",
    "        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n",
    "        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n",
    "\n",
    "        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n",
    "        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n",
    "        pred_w = torch.exp(dw) * widths[:, None]\n",
    "        pred_h = torch.exp(dh) * heights[:, None]\n",
    "\n",
    "        # xmin\n",
    "        pred_boxes1 = pred_ctr_x - torch.tensor(0.5, dtype=pred_ctr_x.dtype, device=pred_w.device) * pred_w\n",
    "        # ymin\n",
    "        pred_boxes2 = pred_ctr_y - torch.tensor(0.5, dtype=pred_ctr_y.dtype, device=pred_h.device) * pred_h\n",
    "        # xmax\n",
    "        pred_boxes3 = pred_ctr_x + torch.tensor(0.5, dtype=pred_ctr_x.dtype, device=pred_w.device) * pred_w\n",
    "        # ymax\n",
    "        pred_boxes4 = pred_ctr_y + torch.tensor(0.5, dtype=pred_ctr_y.dtype, device=pred_h.device) * pred_h\n",
    "        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=2).flatten(1)\n",
    "        return pred_boxes\n",
    "\n",
    "\n",
    "def set_low_quality_matches_(matches, all_matches, match_quality_matrix):\n",
    "    \"\"\"\n",
    "    Produce additional matches for predictions that have only low-quality matches.\n",
    "    Specifically, for each ground-truth find the set of predictions that have\n",
    "    maximum overlap with it (including ties); for each prediction in that set, if\n",
    "    it is unmatched, then match it to the ground-truth with which it has the highest\n",
    "    quality value.\n",
    "    \"\"\"\n",
    "    # For each gt, find the prediction with which it has highest quality\n",
    "    highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)  # the dimension to reduce.\n",
    "\n",
    "    # Find highest quality match available, even if it is low, including ties\n",
    "    gt_pred_pairs_of_highest_quality = torch.nonzero(\n",
    "        match_quality_matrix == highest_quality_foreach_gt[:, None]\n",
    "    )\n",
    "    # Example gt_pred_pairs_of_highest_quality:\n",
    "    #   tensor([[    0, 39796],\n",
    "    #           [    1, 32055],\n",
    "    #           [    1, 32070],\n",
    "    #           [    2, 39190],\n",
    "    #           [    2, 40255],\n",
    "    #           [    3, 40390],\n",
    "    #           [    3, 41455],\n",
    "    #           [    4, 45470],\n",
    "    #           [    5, 45325],\n",
    "    #           [    5, 46390]])\n",
    "    # Each row is a (gt index, prediction index)\n",
    "    # Note how gt items 1, 2, 3, and 5 each have two ties\n",
    "\n",
    "    pre_inds_to_update = gt_pred_pairs_of_highest_quality[:, 1]\n",
    "    matches[pre_inds_to_update] = all_matches[pre_inds_to_update]\n",
    "\n",
    "\n",
    "class Matcher(object):\n",
    "    BELOW_LOW_THRESHOLD = -1\n",
    "    BETWEEN_THRESHOLDS = -2\n",
    "\n",
    "    def __init__(self, high_threshold, low_threshold, allow_low_quality_matches=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            high_threshold (float): quality values greater than or equal to\n",
    "                this value are candidate matches.\n",
    "            low_threshold (float): a lower quality threshold used to stratify\n",
    "                matches into three levels:\n",
    "                1) matches >= high_threshold\n",
    "                2) BETWEEN_THRESHOLDS matches in [low_threshold, high_threshold)\n",
    "                3) BELOW_LOW_THRESHOLD matches in [0, low_threshold)\n",
    "            allow_low_quality_matches (bool): if True, produce additional matches\n",
    "                for predictions that have only low-quality match candidates. See\n",
    "                set_low_quality_matches_ for more details.\n",
    "        \"\"\"\n",
    "        self.BELOW_LOW_THRESHOLD = -1\n",
    "        self.BETWEEN_THRESHOLDS = -2\n",
    "        assert low_threshold <= high_threshold\n",
    "        self.high_threshold = high_threshold  # 0.7\n",
    "        self.low_threshold = low_threshold    # 0.3\n",
    "        self.allow_low_quality_matches = allow_low_quality_matches\n",
    "\n",
    "    def __call__(self, match_quality_matrix):\n",
    "        \"\"\"\n",
    "        calculate maximum iou between anchors and gt boxes, save index\n",
    "        iou < low_threshold: -1\n",
    "        iou > high_threshold: 1\n",
    "        low_threshold<=iou<high_threshold: -2\n",
    "        :param match_quality_matrix:an MxN tensor, containing the\n",
    "            pairwise quality between M ground-truth elements and N predicted elements\n",
    "        :return:  matches (Tensor[int64]): an N tensor where N[i] is a matched gt in\n",
    "            [0, M - 1] or a negative value indicating that prediction i could not\n",
    "            be matched.\n",
    "        \"\"\"\n",
    "\n",
    "        if match_quality_matrix.numel() == 0:\n",
    "            # empty targets or proposals not supported during training\n",
    "            if match_quality_matrix.shape[0] == 0:\n",
    "                raise ValueError(\n",
    "                    \"No ground-truth boxes available for one of the images \"\n",
    "                    \"during training\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"No proposal boxes available for one of the images \"\n",
    "                    \"during training\")\n",
    "\n",
    "        # match_quality_matrix is M (gt) x N (predicted)\n",
    "        # Max over gt elements (dim 0) to find best gt candidate for each prediction\n",
    "        matched_vals, matches = match_quality_matrix.max(dim=0)  # the dimension to reduce.\n",
    "        if self.allow_low_quality_matches:\n",
    "            all_matches = matches.clone()\n",
    "        else:\n",
    "            all_matches = None\n",
    "\n",
    "        # Assign candidate matches with low quality to negative (unassigned) values\n",
    "        below_low_threshold = matched_vals < self.low_threshold\n",
    "        between_thresholds = (matched_vals >= self.low_threshold) & (\n",
    "            matched_vals < self.high_threshold\n",
    "        )\n",
    "        matches[below_low_threshold] = self.BELOW_LOW_THRESHOLD  # -1\n",
    "\n",
    "        matches[between_thresholds] = self.BETWEEN_THRESHOLDS    # -2\n",
    "\n",
    "        if self.allow_low_quality_matches:\n",
    "            assert all_matches is not None\n",
    "            set_low_quality_matches_(matches, all_matches, match_quality_matrix)\n",
    "\n",
    "        return matches\n",
    "\n",
    "\n",
    "def smooth_l1_loss(input, target, beta: float = 1. / 9, size_average: bool = True):\n",
    "    \"\"\"\n",
    "    smooth_l1_loss for bbox regression\n",
    "    :param input:\n",
    "    :param target:\n",
    "    :param beta:\n",
    "    :param size_average:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    n = torch.abs(input - target)\n",
    "    cond = n < beta\n",
    "    loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n",
    "    if size_average:\n",
    "        return loss.mean()\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b87cf0a-8d64-46ef-a174-f952bcf22868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "import numpy as np\n",
    "\n",
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]\n",
    "\n",
    "\n",
    "def filter_low_thresh(boxes, scores, classes, category_index, thresh, box_to_display_str_map, box_to_color_map):\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if scores[i] > thresh:\n",
    "            box = tuple(boxes[i].tolist())\n",
    "            if classes[i] in category_index.keys():\n",
    "                class_name = category_index[classes[i]]\n",
    "            else:\n",
    "                class_name = 'N/A'\n",
    "            display_str = str(class_name)\n",
    "            display_str = '{}: {}%'.format(display_str, int(100 * scores[i]))\n",
    "            box_to_display_str_map[box].append(display_str)\n",
    "            box_to_color_map[box] = STANDARD_COLORS[\n",
    "                classes[i] % len(STANDARD_COLORS)]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "def draw_text(draw, box_to_display_str_map, box, left, right, top, bottom, color):\n",
    "    try:\n",
    "        font = ImageFont.truetype('arial.ttf', 24)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    display_str_heights = [font.getsize(ds)[1] for ds in box_to_display_str_map[box]]\n",
    "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "    if top > total_display_str_height:\n",
    "        text_bottom = top\n",
    "    else:\n",
    "        text_bottom = bottom + total_display_str_height\n",
    "    # Reverse list and print from bottom to top.\n",
    "    for display_str in box_to_display_str_map[box][::-1]:\n",
    "        text_width, text_height = font.getsize(display_str)\n",
    "        margin = np.ceil(0.05 * text_height)\n",
    "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                        (left + text_width, text_bottom)], fill=color)\n",
    "        draw.text((left + margin, text_bottom - text_height - margin),\n",
    "                  display_str,\n",
    "                  fill='black',\n",
    "                  font=font)\n",
    "        text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_box(image, boxes, classes, scores, category_index, thresh=0.5, line_thickness=8):\n",
    "    box_to_display_str_map = collections.defaultdict(list)\n",
    "    box_to_color_map = collections.defaultdict(str)\n",
    "\n",
    "    filter_low_thresh(boxes, scores, classes, category_index, thresh, box_to_display_str_map, box_to_color_map)\n",
    "\n",
    "    # Draw all boxes onto image.\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, color in box_to_color_map.items():\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        (left, right, top, bottom) = (xmin * 1, xmax * 1,\n",
    "                                      ymin * 1, ymax * 1)\n",
    "        draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "                   (right, top), (left, top)], width=line_thickness, fill=color)\n",
    "        draw_text(draw, box_to_display_str_map, box, left, right, top, bottom, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f0a2b0-bd90-47e5-8b65-f198e42e3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "# from utils.train_utils import MetricLogger\n",
    "# from utils.coco_utils import get_coco_api_from_dataset, CocoEvaluator\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, mAP_list=None):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test: \"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = [\"bbox\"]\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for image, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        image = list(img.to(device) for img in image)\n",
    "\n",
    "        if device != torch.device(\"cpu\"):\n",
    "            torch.cuda.synchronize(device)\n",
    "\n",
    "        model_time = time.time()\n",
    "        outputs = model(image)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "\n",
    "    print_txt = coco_evaluator.coco_eval[iou_types[0]].stats\n",
    "    coco_mAP = print_txt[0]\n",
    "    voc_mAP = print_txt[1]\n",
    "    if isinstance(mAP_list, list):\n",
    "        mAP_list.append(voc_mAP)\n",
    "\n",
    "    return coco_evaluator, voc_mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e683c36-f33a-4090-818b-d25422666136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.jit.annotations import Tuple, List, Dict, Optional\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "# from utils.anchor_utils import AnchorsGenerator\n",
    "# from utils.roi_header_util import RoIHeads\n",
    "# from utils.rpn_utils import RPNHead, RegionProposalNetwork\n",
    "# from utils.transform_utils import GeneralizedRCNNTransform\n",
    "\n",
    "\n",
    "class FasterRCNNBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Main class for Generalized R-CNN.\n",
    "\n",
    "    Arguments:\n",
    "        backbone (nn.Module):\n",
    "        rpn (nn.Module):\n",
    "        roi_heads (nn.Module): takes the features + the proposals from the RPN and computes\n",
    "            detections / masks from it.\n",
    "        transform (nn.Module): performs the data transformation from the inputs to feed into\n",
    "            the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, rpn, roi_heads, transform):\n",
    "        super(FasterRCNNBase, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.backbone = backbone\n",
    "        self.rpn = rpn\n",
    "        self.roi_heads = roi_heads\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, losses, detections):\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training and targets is None:\n",
    "            raise ValueError(\"In training mode, targets should be passed\")\n",
    "\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                if isinstance(boxes, torch.Tensor):\n",
    "                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n",
    "                        raise ValueError(\"Expected target boxes to be a tensor\"\n",
    "                                         \"of shape [N, 4], got {:}.\".format(\n",
    "                            boxes.shape))\n",
    "                else:\n",
    "                    raise ValueError(\"Expected target boxes to be of type \"\n",
    "                                     \"Tensor, got {:}.\".format(type(boxes)))\n",
    "\n",
    "        original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            assert len(val) == 2\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        images, targets = self.transform(images, targets)\n",
    "\n",
    "        features = self.backbone(images.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([('0', features)])\n",
    "\n",
    "        proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "\n",
    "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "\n",
    "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "\n",
    "        return self.eager_outputs(losses, detections)\n",
    "\n",
    "\n",
    "class TwoMLPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    two fc layers after roi pooling/align\n",
    "    :param in_channels: number of input channels\n",
    "    :param representation_size: size of the intermediate representation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, representation_size):\n",
    "        super(TwoMLPHead, self).__init__()\n",
    "\n",
    "        self.fc6 = nn.Linear(in_channels, representation_size)\n",
    "        self.fc7 = nn.Linear(representation_size, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FastRCNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers for Fast R-CNN.\n",
    "    :param in_channels: number of input channels\n",
    "    :param num_classes: number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FastRCNNPredictor, self).__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas\n",
    "\n",
    "\n",
    "class FasterRCNN(FasterRCNNBase):\n",
    "    \"\"\"\n",
    "    Implementation of Faster R-CNN.\n",
    "\n",
    "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
    "    image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\n",
    "    The behavior of the model changes depending if it is in training or inference mode.\n",
    "\n",
    "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values\n",
    "          between 0 and H and 0 and W\n",
    "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
    "    follows:\n",
    "        - boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between\n",
    "          0 and H and 0 and W\n",
    "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "        - scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "    :param backbone: (nn.Module), the network used to compute the features for the model.\n",
    "            It should contain a out_channels attribute, which indicates the number of output\n",
    "            channels that each feature map has (and it should be the same for all feature maps).\n",
    "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "    :param num_classes: (int), number of output classes of the model (including the background).\n",
    "            If box_predictor is specified, num_classes should be None.\n",
    "    :param min_size: (int), minimum size of the image to be rescaled before feeding it to the backbone\n",
    "    :param max_size: (int), maximum size of the image to be rescaled before feeding it to the backbone\n",
    "    :param image_mean: (Tuple[float, float, float]):, mean values used for input normalization.\n",
    "            They are generally the mean values of the dataset on which the backbone has been trained\n",
    "            on\n",
    "    :param image_std: (Tuple[float, float, float]), std values used for input normalization.\n",
    "            They are generally the std values of the dataset on which the backbone has been trained on\n",
    "    :param rpn_anchor_generator: (AnchorGenerator), module that generates the anchors for a set of feature maps.\n",
    "    :param rpn_head: (nn.Module),  module that computes the objectness and regression deltas from the RPN\n",
    "    :param rpn_pre_nms_top_n_train:(int),  number of proposals to keep before applying NMS during training\n",
    "    :param rpn_pre_nms_top_n_test: (int), number of proposals to keep before applying NMS during testing\n",
    "    :param rpn_post_nms_top_n_train: (int), number of proposals to keep after applying NMS during training\n",
    "    :param rpn_post_nms_top_n_test: (int), number of proposals to keep after applying NMS during testing\n",
    "    :param rpn_nms_thresh: (float), NMS threshold used for postprocessing the RPN proposals\n",
    "    :param rpn_fg_iou_thresh:(float), minimum IoU between the anchor and the GT box so that they can be\n",
    "            considered as positive during training of the RPN.\n",
    "    :param rpn_bg_iou_thresh:(float), maximum IoU between the anchor and the GT box so that they can be\n",
    "            considered as negative during training of the RPN.\n",
    "    :param rpn_batch_size_per_image: (int), number of anchors that are sampled during training of the RPN\n",
    "            for computing the loss\n",
    "    :param rpn_positive_fraction: (float), proportion of positive anchors in a mini-batch during training\n",
    "            of the RPN\n",
    "    :param box_roi_pool:(MultiScaleRoIAlign), the module which crops and resizes the feature maps in\n",
    "            the locations indicated by the bounding boxes\n",
    "    :param box_head:(nn.Module), module that takes the cropped feature maps as input\n",
    "    :param box_predictor:(nn.Module), module that takes the output of box_head and returns the\n",
    "            classification logits and box regression deltas.\n",
    "    :param box_score_thresh:(float),during inference, only return proposals with a classification score\n",
    "            greater than box_score_thresh\n",
    "    :param box_nms_thresh: (float), NMS threshold for the prediction head. Used during inference\n",
    "    :param box_detections_per_img: (int), maximum number of detections per image, for all classes.\n",
    "    :param box_fg_iou_thresh:(float): minimum IoU between the proposals and the GT box so that they can be\n",
    "            considered as positive during training of the classification head\n",
    "    :param box_bg_iou_thresh: (float), maximum IoU between the proposals and the GT box so that they can be\n",
    "            considered as negative during training of the classification head\n",
    "    :param box_batch_size_per_image: (int), number of proposals that are sampled during training of the\n",
    "            classification head\n",
    "    :param box_positive_fraction: (float), proportion of positive proposals in a mini-batch during training\n",
    "            of the classification head\n",
    "    :param bbox_reg_weights: (Tuple[float, float, float, float]), weights for the encoding/decoding of the\n",
    "            bounding boxes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_classes=None,\n",
    "                 # transform parameter\n",
    "                 min_size=300, max_size=800,  # preprocess minimum and maximum size\n",
    "                 image_mean=None, image_std=None,  # mean and std in preprocess\n",
    "\n",
    "                 # RPN parameters\n",
    "                 rpn_anchor_generator=None, rpn_head=None,\n",
    "                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,  # kept proposals before nms\n",
    "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,  # kept proposals after nms\n",
    "                 rpn_nms_thresh=0.7,  # iou threshold during nms\n",
    "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,  # bg/fg threshold\n",
    "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,  # number of samples and fraction\n",
    "\n",
    "                 # Box parameters\n",
    "                 box_roi_pool=None, box_head=None, box_predictor=None,\n",
    "\n",
    "                 # remove low threshold target\n",
    "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
    "                 bbox_reg_weights=None\n",
    "                 ):\n",
    "\n",
    "        if not hasattr(backbone, \"out_channels\"):\n",
    "            raise ValueError(\n",
    "                \"backbone should contain an attribute out_channels\"\n",
    "                \"specifying the number of output channels  (assumed to be the\"\n",
    "                \"same for all the levels\"\n",
    "            )\n",
    "\n",
    "        assert isinstance(rpn_anchor_generator, (AnchorsGenerator, type(None)))\n",
    "        assert isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None)))\n",
    "\n",
    "        if num_classes is not None:\n",
    "            if box_predictor is not None:\n",
    "                raise ValueError(\"num_classes should be None when box_predictor \"\n",
    "                                 \"is specified\")\n",
    "        else:\n",
    "            if box_predictor is None:\n",
    "                raise ValueError(\"num_classes should not be None when box_predictor \"\n",
    "                                 \"is not specified\")\n",
    "\n",
    "        # output channels of the backbone\n",
    "        out_channels = backbone.out_channels\n",
    "\n",
    "        if rpn_head is None:\n",
    "            rpn_head = RPNHead(\n",
    "                out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
    "            )\n",
    "\n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            rpn_anchor_generator, rpn_head,\n",
    "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)\n",
    "\n",
    "        # two fc layer after roi pooling\n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            representation_size = 1024\n",
    "            box_head = TwoMLPHead(\n",
    "                out_channels * resolution ** 2,\n",
    "                representation_size\n",
    "            )\n",
    "\n",
    "        # get prediction\n",
    "        if box_predictor is None:\n",
    "            representation_size = 1024\n",
    "            box_predictor = FastRCNNPredictor(\n",
    "                representation_size,\n",
    "                num_classes)\n",
    "\n",
    "        roi_heads = RoIHeads(\n",
    "            # box\n",
    "            box_roi_pool, box_head, box_predictor,\n",
    "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
    "            box_batch_size_per_image, box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh, box_nms_thresh, box_detections_per_img)\n",
    "\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
    "\n",
    "        super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25372df9-a451-4e6a-a8b2-a154bc8a9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.jit.annotations import List, Tuple\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            # bbox: xmin, ymin, xmax, ymax\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "# class ImageList(object):\n",
    "#     \"\"\"\n",
    "#     Structure that holds a list of images (of possibly\n",
    "#     varying sizes) as a single tensor.\n",
    "#     This works by padding the images to the same size,\n",
    "#     and storing in a field the original sizes of each image\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, tensors, image_sizes):\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             tensors (tensor) padding\n",
    "#             image_sizes (list[tuple[int, int]])  padding\n",
    "#         \"\"\"\n",
    "#         self.tensors = tensors\n",
    "#         self.image_sizes = image_sizes\n",
    "\n",
    "#     def to(self, device):\n",
    "#         cast_tensor = self.tensors.to(device)\n",
    "#         return ImageList(cast_tensor, self.image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d491ce1d-7109-4ae9-afd8-23b402e84e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_loss_and_lr(train_loss, learning_rate, save_dir):\n",
    "    try:\n",
    "        x = list(range(len(train_loss)))\n",
    "        fig, ax1 = plt.subplots(1, 1)\n",
    "        ax1.plot(x, train_loss, 'r', label='loss')\n",
    "        ax1.set_xlabel(\"step\")\n",
    "        ax1.set_ylabel(\"loss\")\n",
    "        ax1.set_title(\"Train Loss and lr\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(x, learning_rate, label='lr')\n",
    "        ax2.set_ylabel(\"learning rate\")\n",
    "        ax2.set_xlim(0, len(train_loss))\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        plt.legend(handles1 + handles2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        fig.savefig(os.path.join(save_dir, 'loss_and_lr.png'))\n",
    "        plt.close()\n",
    "        print(\"successful save loss curve! \")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def plot_map(mAP, save_dir):\n",
    "    try:\n",
    "        x = list(range(len(mAP)))\n",
    "        plt.plot(x, mAP, label='mAp')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.title('Eval mAP')\n",
    "        plt.xlim(0, len(mAP))\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(os.path.join(save_dir, 'mAP.png'))\n",
    "        plt.close()\n",
    "        print(\"successful save mAP curve!\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0b2c71a-17dc-456c-b20e-6cc9c7be9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import List, Dict, Tuple\n",
    "\n",
    "# from utils.boxes_utils import (\n",
    "#     box_iou,\n",
    "#     clip_boxes_to_image,\n",
    "#     remove_small_boxes,\n",
    "#     batched_nms\n",
    "# )\n",
    "# from utils.det_utils import *\n",
    "\n",
    "\n",
    "def fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    regression_targets = torch.cat(regression_targets, dim=0)\n",
    "\n",
    "    classification_loss = F.cross_entropy(class_logits, labels)\n",
    "\n",
    "    sampled_pos_inds_subset = torch.nonzero(labels > 0).squeeze(1)\n",
    "    labels_pos = labels[sampled_pos_inds_subset]\n",
    "\n",
    "    N, num_classes = class_logits.shape\n",
    "    box_regression = box_regression.reshape(N, -1, 4)\n",
    "\n",
    "    box_loss = smooth_l1_loss(\n",
    "        box_regression[sampled_pos_inds_subset, labels_pos],\n",
    "        regression_targets[sampled_pos_inds_subset],\n",
    "        beta=1 / 9,\n",
    "        size_average=False,\n",
    "    ) / labels.numel()\n",
    "\n",
    "    return classification_loss, box_loss\n",
    "\n",
    "\n",
    "def add_gt_proposals(proposals, gt_boxes):\n",
    "    return [torch.cat((proposal, gt_box)) for proposal, gt_box in zip(proposals, gt_boxes)]\n",
    "\n",
    "\n",
    "def check_targets(targets):\n",
    "    assert targets is not None\n",
    "    assert all([\"boxes\" in t for t in targets])\n",
    "    assert all([\"labels\" in t for t in targets])\n",
    "\n",
    "\n",
    "class RoIHeads(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 box_roi_pool,\n",
    "                 box_head,\n",
    "                 box_predictor,\n",
    "                 fg_iou_thresh, bg_iou_thresh,\n",
    "                 batch_size_per_image, positive_fraction,\n",
    "                 bbox_reg_weights,\n",
    "                 score_thresh,\n",
    "                 nms_thresh,\n",
    "                 detection_per_img):\n",
    "        super(RoIHeads, self).__init__()\n",
    "\n",
    "        self.box_similarity = box_iou\n",
    "\n",
    "        self.proposal_matcher = Matcher(\n",
    "            fg_iou_thresh, bg_iou_thresh, allow_low_quality_matches=False)\n",
    "\n",
    "        self.fg_bg_sampler = BalancedPositiveNegativeSampler(\n",
    "            batch_size_per_image, positive_fraction)\n",
    "\n",
    "        if bbox_reg_weights is None:\n",
    "            bbox_reg_weights = (10., 10., 5., 5.)\n",
    "        self.box_coder = BoxCoder(bbox_reg_weights)\n",
    "\n",
    "        self.box_roi_pool = box_roi_pool\n",
    "        self.box_head = box_head\n",
    "        self.box_predictor = box_predictor\n",
    "\n",
    "        self.score_thresh = score_thresh\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.detection_per_img = detection_per_img\n",
    "\n",
    "    def assign_targets_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        matched_idxs = []\n",
    "        labels = []\n",
    "        for proposals_in_image, gt_boxes_in_image, gt_labels_in_image in zip(proposals, gt_boxes, gt_labels):\n",
    "            if gt_boxes_in_image.numel() == 0:\n",
    "                device = proposals_in_image.device\n",
    "                clamped_matched_idxs_in_image = torch.zeros(\n",
    "                    (proposals_in_image.shape[0],), dtype=torch.int64, device=device)\n",
    "                labels_in_image = torch.zeros(\n",
    "                    (proposals_in_image.shape[0],), dtype=torch.int64, device=device)\n",
    "            else:\n",
    "                match_quality_matrix = box_iou(gt_boxes_in_image, proposals_in_image)\n",
    "                matched_idxs_in_image = self.proposal_matcher(match_quality_matrix)\n",
    "                clamped_matched_idxs_in_image = matched_idxs_in_image.clamp(min=0)\n",
    "\n",
    "                labels_in_image = gt_labels_in_image[clamped_matched_idxs_in_image]\n",
    "                labels_in_image = labels_in_image.to(dtype=torch.int64)\n",
    "\n",
    "                bg_inds = matched_idxs_in_image == self.proposal_matcher.BELOW_LOW_THRESHOLD\n",
    "                labels_in_image[bg_inds] = 0\n",
    "\n",
    "                ignore_inds = matched_idxs_in_image == self.proposal_matcher.BETWEEN_THRESHOLDS\n",
    "                labels_in_image[ignore_inds] = -1\n",
    "\n",
    "            matched_idxs.append(clamped_matched_idxs_in_image)\n",
    "            labels.append(labels_in_image)\n",
    "        return matched_idxs, labels\n",
    "\n",
    "    def subsample(self, labels):\n",
    "        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n",
    "        sampled_inds = []\n",
    "        for img_idx, (pos_inds_img, neg_inds_img) in enumerate(zip(sampled_pos_inds, sampled_neg_inds)):\n",
    "            img_sampled_inds = torch.nonzero(pos_inds_img | neg_inds_img).squeeze(1)\n",
    "            sampled_inds.append(img_sampled_inds)\n",
    "        return sampled_inds\n",
    "\n",
    "    def select_training_samples(self, proposals, targets):\n",
    "        check_targets(targets)\n",
    "        dtype = proposals[0].dtype\n",
    "        device = proposals[0].device\n",
    "\n",
    "        gt_boxes = [t[\"boxes\"].to(dtype) for t in targets]\n",
    "        gt_labels = [t[\"labels\"] for t in targets]\n",
    "\n",
    "        proposals = add_gt_proposals(proposals, gt_boxes)\n",
    "\n",
    "        matched_idxs, labels = self.assign_targets_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "\n",
    "        sampled_inds = self.subsample(labels)\n",
    "        matched_gt_boxes = []\n",
    "        num_images = len(proposals)\n",
    "\n",
    "        for img_id in range(num_images):\n",
    "            img_sampled_inds = sampled_inds[img_id]\n",
    "            proposals[img_id] = proposals[img_id][img_sampled_inds]\n",
    "            labels[img_id] = labels[img_id][img_sampled_inds]\n",
    "            matched_idxs[img_id] = matched_idxs[img_id][img_sampled_inds]\n",
    "\n",
    "            gt_boxes_in_image = gt_boxes[img_id]\n",
    "            if gt_boxes_in_image.numel() == 0:\n",
    "                gt_boxes_in_image = torch.zeros((1, 4), dtype=dtype, device=device)\n",
    "            matched_gt_boxes.append(gt_boxes_in_image[matched_idxs[img_id]])\n",
    "\n",
    "        regression_targets = self.box_coder.encode(matched_gt_boxes, proposals)\n",
    "        return proposals, matched_idxs, labels, regression_targets\n",
    "\n",
    "    def postprocess_detections(self, class_logits, box_regression, proposals, image_shapes):\n",
    "        device = class_logits.device\n",
    "        num_classes = class_logits.shape[-1]\n",
    "\n",
    "        boxes_per_image = [boxes.shape[0] for boxes in proposals]\n",
    "        pred_boxes = self.box_coder.decode(box_regression, proposals)\n",
    "        pred_scores = F.softmax(class_logits, -1)\n",
    "\n",
    "        pred_boxes_list = pred_boxes.split(boxes_per_image, 0)\n",
    "        pred_scores_list = pred_scores.split(boxes_per_image, 0)\n",
    "\n",
    "        all_boxes, all_scores, all_labels = [], [], []\n",
    "        for boxes, scores, image_shape in zip(pred_boxes_list, pred_scores_list, image_shapes):\n",
    "            boxes = clip_boxes_to_image(boxes, image_shape)\n",
    "\n",
    "            labels = torch.arange(num_classes, device=device).view(1, -1).expand_as(scores)\n",
    "\n",
    "            boxes = boxes[:, 1:]\n",
    "            scores = scores[:, 1:]\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "            boxes = boxes.reshape(-1, 4)\n",
    "            scores = scores.reshape(-1)\n",
    "            labels = labels.reshape(-1)\n",
    "\n",
    "            inds = torch.nonzero(scores > self.score_thresh).squeeze(1)\n",
    "            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
    "\n",
    "            keep = remove_small_boxes(boxes, min_size=1e-2)\n",
    "            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
    "\n",
    "            keep = batched_nms(boxes, scores, labels, self.nms_thresh)\n",
    "\n",
    "            keep = keep[:self.detection_per_img]\n",
    "            all_boxes.append(boxes[keep])\n",
    "            all_scores.append(scores[keep])\n",
    "            all_labels.append(labels[keep])\n",
    "\n",
    "        return all_boxes, all_scores, all_labels\n",
    "\n",
    "    def forward(self, features, proposals, image_shapes, targets=None):\n",
    "        if targets is not None:\n",
    "            for t in targets:\n",
    "                floating_point_types = (torch.float, torch.double, torch.half)\n",
    "                assert t[\"boxes\"].dtype in floating_point_types\n",
    "\n",
    "        if self.training:\n",
    "            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n",
    "        else:\n",
    "            labels = None\n",
    "            regression_targets = None\n",
    "            matched_idxs = None\n",
    "\n",
    "        box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
    "        box_features = self.box_head(box_features)\n",
    "        class_logits, box_regression = self.box_predictor(box_features)\n",
    "\n",
    "        result = torch.jit.annotate(List[Dict[str, torch.Tensor]], [])\n",
    "        losses = {}\n",
    "\n",
    "        if self.training:\n",
    "            assert labels is not None and regression_targets is not None\n",
    "            loss_classifier, loss_box_reg = fastrcnn_loss(\n",
    "                class_logits, box_regression, labels, regression_targets)\n",
    "            losses = {\n",
    "                \"loss_classifier\": loss_classifier,\n",
    "                \"loss_box_reg\": loss_box_reg\n",
    "            }\n",
    "        else:\n",
    "            boxes, scores, labels = self.postprocess_detections(\n",
    "                class_logits, box_regression, proposals, image_shapes)\n",
    "            for i in range(len(boxes)):\n",
    "                result.append({\n",
    "                    \"boxes\": boxes[i],\n",
    "                    \"labels\": labels[i],\n",
    "                    \"scores\": scores[i]\n",
    "                })\n",
    "\n",
    "        return result, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4d663eb-1404-4b10-a457-05cefd81f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.jit.annotations import Dict\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# import utils.boxes_utils as box_op\n",
    "# from utils.det_utils import *\n",
    "\n",
    "\n",
    "# from torch import nn\n",
    "# from torch.jit.annotations import Dict\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# from utils.boxes_utils import (\n",
    "#     box_iou,\n",
    "#     clip_boxes_to_image,\n",
    "#     remove_small_boxes,\n",
    "#     batched_nms,\n",
    "#     concat_box_prediction_layers\n",
    "# )\n",
    "# from utils.det_utils import *\n",
    "\n",
    "\n",
    "class RPNHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors):\n",
    "        super(RPNHead, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1)\n",
    "\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "                torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_scores = []\n",
    "        bbox_reg = []\n",
    "        for i, feature in enumerate(x):\n",
    "            t = F.relu(self.conv(feature))\n",
    "            cls_scores.append(self.cls_logits(t))\n",
    "            bbox_reg.append(self.bbox_pred(t))\n",
    "        return cls_scores, bbox_reg\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(torch.nn.Module):\n",
    "    def __init__(self, anchor_generator, head, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction,\n",
    "                 pre_nms_top_n, post_nms_top_n, nms_thresh):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.head = head\n",
    "        self.box_coder = BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n",
    "        self.box_similarity = box_iou\n",
    "\n",
    "        self.proposal_matcher = Matcher(\n",
    "            fg_iou_thresh,\n",
    "            bg_iou_thresh,\n",
    "            allow_low_quality_matches=True\n",
    "        )\n",
    "\n",
    "        self.fg_bg_sampler = BalancedPositiveNegativeSampler(\n",
    "            batch_size_per_image, positive_fraction\n",
    "        )\n",
    "\n",
    "        self._pre_nms_top_n = pre_nms_top_n\n",
    "        self._post_nms_top_n = post_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.min_size = 1e-3\n",
    "\n",
    "    def pre_nms_top_n(self):\n",
    "        return self._pre_nms_top_n['training'] if self.training else self._pre_nms_top_n['testing']\n",
    "\n",
    "    def post_nms_top_n(self):\n",
    "        return self._post_nms_top_n['training'] if self.training else self._post_nms_top_n['testing']\n",
    "\n",
    "    def assign_targets_to_anchors(self, anchors, targets):\n",
    "        labels = []\n",
    "        matched_gt_boxes = []\n",
    "        for anchors_per_image, targets_per_image in zip(anchors, targets):\n",
    "            gt_boxes = targets_per_image[\"boxes\"]\n",
    "            if gt_boxes.numel() == 0:\n",
    "                device = anchors_per_image.device\n",
    "                matched_gt_boxes_per_image = torch.zeros(anchors_per_image.shape, dtype=torch.float32, device=device)\n",
    "                labels_per_image = torch.zeros((anchors_per_image.shape[0],), dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                match_quality_matrix = box_iou(gt_boxes, anchors_per_image)\n",
    "                matched_idxs = self.proposal_matcher(match_quality_matrix)\n",
    "                matched_gt_boxes_per_image = gt_boxes[matched_idxs.clamp(min=0)]\n",
    "\n",
    "                labels_per_image = matched_idxs >= 0\n",
    "                labels_per_image = labels_per_image.to(dtype=torch.float32)\n",
    "\n",
    "                bg_indices = matched_idxs == self.proposal_matcher.BELOW_LOW_THRESHOLD\n",
    "                labels_per_image[bg_indices] = 0.0\n",
    "\n",
    "                inds_to_discard = matched_idxs == self.proposal_matcher.BETWEEN_THRESHOLDS\n",
    "                labels_per_image[inds_to_discard] = -1.0\n",
    "\n",
    "            labels.append(labels_per_image)\n",
    "            matched_gt_boxes.append(matched_gt_boxes_per_image)\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def _get_top_n_idx(self, objectness, num_anchors_per_level):\n",
    "        result = []\n",
    "        offset = 0\n",
    "        for ob in objectness.split(num_anchors_per_level, 1):\n",
    "            num_anchors = ob.shape[1]\n",
    "            pre_nms_top_n = min(self.pre_nms_top_n(), num_anchors)\n",
    "            _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)\n",
    "            result.append(top_n_idx + offset)\n",
    "            offset += num_anchors\n",
    "        return torch.cat(result, dim=1)\n",
    "\n",
    "    def filter_proposals(self, proposals, objectness, image_shapes, num_anchors_per_level):\n",
    "        num_images = proposals.shape[0]\n",
    "        device = proposals.device\n",
    "\n",
    "        objectness = objectness.detach().reshape(num_images, -1)\n",
    "\n",
    "        levels = [torch.full((n,), idx, dtype=torch.int64, device=device)\n",
    "                  for idx, n in enumerate(num_anchors_per_level)]\n",
    "        levels = torch.cat(levels, 0).reshape(1, -1).expand_as(objectness)\n",
    "\n",
    "        top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)\n",
    "\n",
    "        image_range = torch.arange(num_images, device=device)\n",
    "        batch_idx = image_range[:, None]\n",
    "\n",
    "        objectness = objectness[batch_idx, top_n_idx]\n",
    "        levels = levels[batch_idx, top_n_idx]\n",
    "        proposals = proposals[batch_idx, top_n_idx]\n",
    "\n",
    "        final_boxes = []\n",
    "        final_scores = []\n",
    "        for boxes, scores, lvl, img_shape in zip(proposals, objectness, levels, image_shapes):\n",
    "            boxes = clip_boxes_to_image(boxes, img_shape)\n",
    "            keep = remove_small_boxes(boxes, self.min_size)\n",
    "            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]\n",
    "\n",
    "            keep = batched_nms(boxes, scores, lvl, self.nms_thresh)\n",
    "            keep = keep[:self.post_nms_top_n()]\n",
    "            final_boxes.append(boxes[keep])\n",
    "            final_scores.append(scores[keep])\n",
    "        return final_boxes, final_scores\n",
    "\n",
    "    def compute_loss(self, objectness, pred_bbox_deltas, labels, regression_targets):\n",
    "        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n",
    "        sampled_pos_inds = torch.nonzero(torch.cat(sampled_pos_inds, dim=0)).squeeze(1)\n",
    "        sampled_neg_inds = torch.nonzero(torch.cat(sampled_neg_inds, dim=0)).squeeze(1)\n",
    "        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n",
    "\n",
    "        objectness = objectness.flatten()\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        regression_targets = torch.cat(regression_targets, dim=0)\n",
    "\n",
    "        box_loss = smooth_l1_loss(\n",
    "            pred_bbox_deltas[sampled_pos_inds],\n",
    "            regression_targets[sampled_pos_inds],\n",
    "            beta=1 / 9,\n",
    "            size_average=False\n",
    "        ) / sampled_inds.numel()\n",
    "\n",
    "        objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "            objectness[sampled_inds],\n",
    "            labels[sampled_inds]\n",
    "        )\n",
    "\n",
    "        return objectness_loss, box_loss\n",
    "\n",
    "    def forward(self, images, features, targets=None):\n",
    "        features = list(features.values())\n",
    "        fg_bg_scores, pred_bbox_deltas = self.head(features)\n",
    "        anchors = self.anchor_generator(images, features)\n",
    "\n",
    "        num_images = len(anchors)\n",
    "        num_anchors_per_level_shape_tensors = [o[0].shape for o in fg_bg_scores]\n",
    "        num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "\n",
    "        fg_bg_scores, pred_bbox_deltas = concat_box_prediction_layers(fg_bg_scores, pred_bbox_deltas)\n",
    "\n",
    "        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "        proposals = proposals.view(num_images, -1, 4)\n",
    "\n",
    "        boxes, scores = self.filter_proposals(proposals, fg_bg_scores, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "        losses = {}\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            labels, matched_gt_boxes = self.assign_targets_to_anchors(anchors, targets)\n",
    "            regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)\n",
    "            loss_objectness, loss_rpn_box_reg = self.compute_loss(\n",
    "                fg_bg_scores, pred_bbox_deltas, labels, regression_targets\n",
    "            )\n",
    "            losses = {\n",
    "                \"loss_objectness\": loss_objectness,\n",
    "                \"loss_rpn_box_reg\": loss_rpn_box_reg\n",
    "            }\n",
    "\n",
    "        return boxes, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c39ff39f-916d-442d-b99b-5338087b2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torchvision import ops\n",
    "\n",
    "# from backbone.mobilenet import MobileNetV2\n",
    "# from backbone.resnet50_fpn_model import *\n",
    "# from config.train_config import cfg\n",
    "# from utils.anchor_utils import AnchorsGenerator\n",
    "# from utils.faster_rcnn_utils import FasterRCNN, FastRCNNPredictor\n",
    "\n",
    "\n",
    "def create_model(num_classes):\n",
    "    global backbone, model\n",
    "    backbone_network = cfg.backbone\n",
    "    if backbone_network == 'mobilenet':\n",
    "        anchor_sizes = ((128, 256, 512),)\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),)\n",
    "    else:  # resnet50_fpn\n",
    "        anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),) * 5\n",
    "    anchor_generator = AnchorsGenerator(sizes=anchor_sizes,\n",
    "                                        aspect_ratios=aspect_ratios)\n",
    "\n",
    "    if backbone_network == 'mobilenet':\n",
    "        backbone = MobileNetV2(weights_path=cfg.backbone_pretrained_weights).features\n",
    "        backbone.out_channels = 1280\n",
    "\n",
    "        roi_pooler = ops.MultiScaleRoIAlign(featmap_names=['0'],  # roi pooling in which resolution feature\n",
    "                                            output_size=cfg.roi_out_size,  # roi_pooling output feature size\n",
    "                                            sampling_ratio=cfg.roi_sample_rate)  # sampling_ratio\n",
    "\n",
    "        model = FasterRCNN(backbone=backbone, num_classes=num_classes,\n",
    "                           # transform parameters\n",
    "                           min_size=cfg.min_size, max_size=cfg.max_size,\n",
    "                           image_mean=cfg.image_mean, image_std=cfg.image_std,\n",
    "                           # rpn parameters\n",
    "                           rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler,\n",
    "                           rpn_pre_nms_top_n_train=cfg.rpn_pre_nms_top_n_train,\n",
    "                           rpn_pre_nms_top_n_test=cfg.rpn_pre_nms_top_n_test,\n",
    "                           rpn_post_nms_top_n_train=cfg.rpn_post_nms_top_n_train,\n",
    "                           rpn_post_nms_top_n_test=cfg.rpn_post_nms_top_n_test,\n",
    "                           rpn_nms_thresh=cfg.rpn_nms_thresh,\n",
    "                           rpn_fg_iou_thresh=cfg.rpn_fg_iou_thresh,\n",
    "                           rpn_bg_iou_thresh=cfg.rpn_bg_iou_thresh,\n",
    "                           rpn_batch_size_per_image=cfg.rpn_batch_size_per_image,\n",
    "                           rpn_positive_fraction=cfg.rpn_positive_fraction,\n",
    "                           # Box parameters\n",
    "                           box_head=None, box_predictor=None,\n",
    "\n",
    "                           # remove low threshold target\n",
    "                           box_score_thresh=cfg.box_score_thresh,\n",
    "                           box_nms_thresh=cfg.box_nms_thresh,\n",
    "                           box_detections_per_img=cfg.box_detections_per_img,\n",
    "                           box_fg_iou_thresh=cfg.box_fg_iou_thresh,\n",
    "                           box_bg_iou_thresh=cfg.box_bg_iou_thresh,\n",
    "                           box_batch_size_per_image=cfg.box_batch_size_per_image,\n",
    "                           box_positive_fraction=cfg.box_positive_fraction,\n",
    "                           bbox_reg_weights=cfg.bbox_reg_weights\n",
    "                           )\n",
    "    elif backbone_network == 'resnet50_fpn':\n",
    "        backbone = resnet50_fpn_backbone()\n",
    "\n",
    "        roi_pooler = ops.MultiScaleRoIAlign(\n",
    "            featmap_names=['0', '1', '2', '3'],\n",
    "            output_size=cfg.roi_out_size,\n",
    "            sampling_ratio=cfg.roi_sample_rate)\n",
    "        model = FasterRCNN(backbone=backbone, num_classes=num_classes,\n",
    "                           # transform parameters\n",
    "                           min_size=cfg.min_size, max_size=cfg.max_size,\n",
    "                           image_mean=cfg.image_mean, image_std=cfg.image_std,\n",
    "                           # rpn parameters\n",
    "                           rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler,\n",
    "                           rpn_pre_nms_top_n_train=cfg.rpn_pre_nms_top_n_train,\n",
    "                           rpn_pre_nms_top_n_test=cfg.rpn_pre_nms_top_n_test,\n",
    "                           rpn_post_nms_top_n_train=cfg.rpn_post_nms_top_n_train,\n",
    "                           rpn_post_nms_top_n_test=cfg.rpn_post_nms_top_n_test,\n",
    "                           rpn_nms_thresh=cfg.rpn_nms_thresh,\n",
    "                           rpn_fg_iou_thresh=cfg.rpn_fg_iou_thresh,\n",
    "                           rpn_bg_iou_thresh=cfg.rpn_bg_iou_thresh,\n",
    "                           rpn_batch_size_per_image=cfg.rpn_batch_size_per_image,\n",
    "                           rpn_positive_fraction=cfg.rpn_positive_fraction,\n",
    "                           # Box parameters\n",
    "                           box_head=None, box_predictor=None,\n",
    "\n",
    "                           # remove low threshold target\n",
    "                           box_score_thresh=cfg.box_score_thresh,\n",
    "                           box_nms_thresh=cfg.box_nms_thresh,\n",
    "                           box_detections_per_img=cfg.box_detections_per_img,\n",
    "                           box_fg_iou_thresh=cfg.box_fg_iou_thresh,\n",
    "                           box_bg_iou_thresh=cfg.box_bg_iou_thresh,\n",
    "                           box_batch_size_per_image=cfg.box_batch_size_per_image,\n",
    "                           box_positive_fraction=cfg.box_positive_fraction,\n",
    "                           bbox_reg_weights=cfg.bbox_reg_weights\n",
    "                           )\n",
    "\n",
    "        # weights_dict = torch.load(cfg.pretrained_weights)\n",
    "        # missing_keys, unexpected_keys = model.load_state_dict(weights_dict, strict=False)\n",
    "        # if len(missing_keys) != 0 or len(unexpected_keys) != 0:\n",
    "        #     print(\"missing_keys: \", missing_keys)\n",
    "        #     print(\"unexpected_keys: \", unexpected_keys)\n",
    "\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=f)\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "        return reduced_dict\n",
    "\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)  # dequelist\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
    "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = \"\"\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([header,\n",
    "                                           '[{0' + space_fmt + '}/{1}]',\n",
    "                                           'eta: {eta}',\n",
    "                                           '{meters}',\n",
    "                                           'time: {time}',\n",
    "                                           'data: {data}',\n",
    "                                           'max mem: {memory:.0f}'])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([header,\n",
    "                                           '[{0' + space_fmt + '}/{1}]',\n",
    "                                           'eta: {eta}',\n",
    "                                           '{meters}',\n",
    "                                           'time: {time}',\n",
    "                                           'data: {data}'])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_second = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=eta_second))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(i, len(iterable),\n",
    "                                         eta=eta_string,\n",
    "                                         meters=str(self),\n",
    "                                         time=str(iter_time),\n",
    "                                         data=str(data_time),\n",
    "                                         memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(i, len(iterable),\n",
    "                                         eta=eta_string,\n",
    "                                         meters=str(self),\n",
    "                                         time=str(iter_time),\n",
    "                                         data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(header,\n",
    "                                                         total_time_str,\n",
    "\n",
    "                                                         total_time / len(iterable)))\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq,\n",
    "                    train_loss=None, train_lr=None, warmup=False):\n",
    "    global loss_dict, losses\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0 and warmup is True:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purpose\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "        if isinstance(train_loss, list):\n",
    "            train_loss.append(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        now_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=now_lr)\n",
    "        if isinstance(train_lr, list):\n",
    "            train_lr.append(now_lr)\n",
    "\n",
    "    return loss_dict, losses\n",
    "\n",
    "\n",
    "def write_tb(writer, num, info):\n",
    "    for item in info.items():\n",
    "        writer.add_scalar(item[0], item[1], num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "610df81a-c88d-4d08-aabe-f72db8923284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.jit.annotations import List, Tuple\n",
    "\n",
    "# from utils.im_utils import ImageList\n",
    "\n",
    "\n",
    "def torch_choice(l):\n",
    "    index = int(torch.empty(1).uniform_(0., float(len(l))).item())\n",
    "    return l[index]\n",
    "\n",
    "\n",
    "def max_by_axis(the_list):\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "\n",
    "def batch_images(images, size_divisible=32):\n",
    "    \"\"\"\n",
    "    batched images\n",
    "    :param images: a set of images\n",
    "    :param size_divisible: ratio of height/width to be adjusted\n",
    "    :return: batched tensor image\n",
    "    \"\"\"\n",
    "\n",
    "    max_size = max_by_axis([list(img.shape) for img in images])\n",
    "\n",
    "    stride = float(size_divisible)\n",
    "\n",
    "    max_size[1] = int(math.ceil(float(max_size[1]) / stride) * stride)\n",
    "    max_size[2] = int(math.ceil(float(max_size[2]) / stride) * stride)\n",
    "\n",
    "    # [batch, channel, height, width]\n",
    "    batch_shape = [len(images)] + max_size\n",
    "\n",
    "    batched_imgs = images[0].new_full(batch_shape, 0)\n",
    "    for img, pad_img in zip(images, batched_imgs):\n",
    "        pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "\n",
    "    return batched_imgs\n",
    "\n",
    "\n",
    "class GeneralizedRCNNTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs input / target transformation before feeding the data to a GeneralizedRCNN model.\n",
    "    The transformations it perform are:\n",
    "        - input normalization (mean subtraction and std division)\n",
    "        - input / target resizing to match min_size / max_size\n",
    "\n",
    "    It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets\n",
    "    :param min_size: minimum size of input image\n",
    "    :param max_size: maximum size of input image\n",
    "    :param image_mean: image mean\n",
    "    :param image_std: image std\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_size, max_size, image_mean, image_std):\n",
    "        super(GeneralizedRCNNTransform, self).__init__()\n",
    "        if not isinstance(min_size, (list, tuple)):\n",
    "            min_size = (min_size,)\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "\n",
    "    def normalize(self, image):\n",
    "        dtype, device = image.dtype, image.device\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        return (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "    def resize(self, image, target):\n",
    "        \"\"\"\n",
    "        resize input image to specified size and transform for target\n",
    "        :param image: input image\n",
    "        :param target: target related info, like bbox\n",
    "        :return:\n",
    "            image: resized image\n",
    "            target: resized target\n",
    "        \"\"\"\n",
    "\n",
    "        # image shape is [channel, height, width]\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = float(torch.min(im_shape))\n",
    "        max_size = float(torch.max(im_shape))\n",
    "        if self.training:\n",
    "            size = float(torch_choice(self.min_size))\n",
    "        else:\n",
    "            size = float(self.min_size[-1])\n",
    "        scale_factor = size / min_size\n",
    "\n",
    "        if max_size * scale_factor > self.max_size:\n",
    "            scale_factor = self.max_size / max_size\n",
    "\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image[None], scale_factor=scale_factor, mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "        if target is None:\n",
    "            return image, target\n",
    "\n",
    "        bbox = target[\"boxes\"]\n",
    "        bbox = resize_boxes(bbox, (h, w), image.shape[-2:])\n",
    "        target[\"boxes\"] = bbox\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def postprocess(self, result, image_shapes, original_image_sizes):\n",
    "        \"\"\"\n",
    "        post process of predictions, mainly map bboxed coordinates to original image\n",
    "        :param result: predictions result\n",
    "        :param image_shapes: image size after preprocess\n",
    "        :param original_image_sizes: original image size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            return result\n",
    "        for i, (pred, im_s, o_im_s) in enumerate(zip(result, image_shapes, original_image_sizes)):\n",
    "            boxes = pred[\"boxes\"]\n",
    "            boxes = resize_boxes(boxes, im_s, o_im_s)\n",
    "            result[i][\"boxes\"] = boxes\n",
    "        return result\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        images = [img for img in images]\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            target_index = targets[i] if targets is not None else None\n",
    "\n",
    "            if image.dim() != 3:\n",
    "                raise ValueError(\"images is expected to be a list of 3d tensors \"\n",
    "                                 \"of shape [C, H, W], got {}\".format(image.shape))\n",
    "            image = self.normalize(image)\n",
    "            image, target_index = self.resize(image, target_index)\n",
    "            images[i] = image\n",
    "            if targets is not None and target_index is not None:\n",
    "                targets[i] = target_index\n",
    "\n",
    "        # save resized image size\n",
    "        image_sizes = [img.shape[-2:] for img in images]\n",
    "        images = batch_images(images)\n",
    "        image_sizes_list = torch.jit.annotate(List[Tuple[int, int]], [])\n",
    "\n",
    "        for image_size in image_sizes:\n",
    "            assert len(image_size) == 2\n",
    "            image_sizes_list.append((image_size[0], image_size[1]))\n",
    "\n",
    "        image_list = ImageList(images, image_sizes_list)\n",
    "        return image_list, targets\n",
    "\n",
    "\n",
    "def resize_boxes(boxes, original_size, new_size):\n",
    "    \"\"\"\n",
    "    resize bbox to original image based on stride\n",
    "    :param boxes: predicted bboxes\n",
    "    :param original_size: original image size\n",
    "    :param new_size: rescaled image size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ratios = [\n",
    "        torch.tensor(s, dtype=torch.float32, device=boxes.device) /\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratios_height, ratios_width = ratios\n",
    "\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratios_width\n",
    "    xmax = xmax * ratios_width\n",
    "    ymin = ymin * ratios_height\n",
    "    ymax = ymax * ratios_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0ddda60-f13e-447a-9404-a65090dee9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors_test():\n",
    "    scales = [64, 128, 256]\n",
    "    ratios = [0.5, 1.0, 2.0]\n",
    "    generate_anchors(scales, ratios)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_anchors_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a60a7ac9-e99a-4921-a673-189a583d6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
    "    '''\n",
    "        Resize image to (300, 300)  for SSD300\n",
    "        image: A PIL image\n",
    "        boxes: bounding boxes, a tensor of dimensions (n_objects, 4)\n",
    "        \n",
    "        Out:New image, new boxes or percent coordinates\n",
    "    '''\n",
    "    if type(image) != PIL.Image.Image:\n",
    "        image = TF.to_pil_image(image)\n",
    "    new_image= TF.resize(image, dims)\n",
    "\n",
    "    # Resize bounding boxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6600acb4-66e4-4fdb-a259-0d30267d87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    '''\n",
    "        Apply transformation\n",
    "        image: A PIL image\n",
    "        boxes: bounding boxe, a tensor of dimensions (n_objects, 4)\n",
    "        labels: labels of object a tensor of dimensions (n_object)\n",
    "        difficulties: difficulties of object detect, a tensor of dimensions (n_object)\n",
    "        split: one of \"TRAIN\", \"TEST\"\n",
    "        \n",
    "        Out: transformed images, transformed bounding boxes, transformed labels,\n",
    "        transformed difficulties\n",
    "    '''\n",
    "    \n",
    "    if type(image) != PIL.Image.Image:\n",
    "        image = TF.to_pil_image(image)\n",
    "    split = split.upper()\n",
    "    if split not in {\"TRAIN\", \"TEST\"}:\n",
    "        print(\"Param split in transform not in {TRAIN, TEST}\")\n",
    "        assert split in {\"TRAIN\", \"TEST\"}\n",
    "    \n",
    "    #mean and std from ImageNet\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "        \n",
    "    #Resize image to (300, 300)\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims= (300, 300))\n",
    "        \n",
    "    new_image = TF.to_tensor(new_image)\n",
    "    new_image = TF.normalize(new_image, mean=mean, std=std)\n",
    "    \n",
    "    return new_image, new_boxes, new_labels, new_difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffa50bbd-856b-4588-93c7-8df581d97292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "label_map = {'UAV': 1, 'background': 0}\n",
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    \n",
    "    for object in root.iter(\"object\"):\n",
    "        difficult = int(object.find(\"difficult\").text == \"1\")\n",
    "        label = object.find(\"name\").text.strip()\n",
    "        if label not in label_map:\n",
    "            print(\"{0} not in label map.\".format(label))\n",
    "            assert label in label_map\n",
    "            \n",
    "        bbox =  object.find(\"bndbox\")\n",
    "        xmin = int(bbox.find(\"xmin\").text)\n",
    "        ymin = int(bbox.find(\"ymin\").text)\n",
    "        xmax = int(bbox.find(\"xmax\").text)\n",
    "        ymax = int(bbox.find(\"ymax\").text)\n",
    "        \n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "        \n",
    "    return {\"boxes\": boxes, \"labels\": labels, \"difficulties\": difficulties}\n",
    "\n",
    "\n",
    "class coco(Dataset):\n",
    "    def __init__(self, base_dir, transforms=None):\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transforms\n",
    "\n",
    "        # Classes\n",
    "        self._classes = ('__background__', 'UAV')\n",
    "        self.classes = self._classes\n",
    "        self.num_classes = len(self.classes)\n",
    "        self._class_to_ind = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Load image and XML paths\n",
    "        self.images = sorted(glob.glob(os.path.join(base_dir, 'img', '*.jpg')) +\n",
    "                             glob.glob(os.path.join(base_dir, 'img', '*.png')) +\n",
    "                             glob.glob(os.path.join(base_dir, 'img', '*.jpeg')))\n",
    "        self.xmls = sorted(glob.glob(os.path.join(base_dir, 'xml', '*.xml')))\n",
    "        assert len(self.images) == len(self.xmls), \"Mismatch between image and annotation count\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        annotation = parse_annotation(self.xmls[idx])\n",
    "\n",
    "        boxes = torch.FloatTensor(annotation[\"boxes\"])\n",
    "        labels = torch.LongTensor(annotation[\"labels\"])\n",
    "        difficulties = torch.ByteTensor(annotation[\"difficulties\"])\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        # area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        # iscrowd = difficulties\n",
    "\n",
    "        # target = {\n",
    "        #     \"boxes\": boxes,\n",
    "        #     \"labels\": labels,\n",
    "        #     \"image_id\": image_id,\n",
    "        #     \"area\": area,\n",
    "        #     \"iscrowd\": iscrowd\n",
    "        # }\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes, labels, difficulties = self.transform(image, boxes, labels, difficulties,split='train')\n",
    "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        iscrowd = difficulties\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    @property\n",
    "    def class_to_coco_cat_id(self):\n",
    "        return {cls: idx for cls, idx in self._class_to_ind.items() if cls != '__background__'}\n",
    "\n",
    "def combine(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for image, target in batch:\n",
    "        if isinstance(image, Image.Image):  # Convert to tensor if still PIL\n",
    "            image = TF.to_tensor(image)\n",
    "    \n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    \n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# def combine(batch):\n",
    "#     images = []\n",
    "#     targets = []\n",
    "    \n",
    "#     for image, target in batch:\n",
    "#         if isinstance(image, Image.Image):\n",
    "#             image = TF.to_tensor(image)\n",
    "#         images.append(image)\n",
    "#         targets.append(target)\n",
    "    \n",
    "#     # Don't stack the images; return as list or tuple\n",
    "#     return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae1f1ee4-d1b9-4875-8b1e-a9fd2bdd4e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 dataloader workers\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "train_data_set = coco('train/train',transform)\n",
    "\n",
    "# Set batch size and determine optimal number of workers\n",
    "batch_size = cfg.batch_size\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 4])  # Max 8 workers\n",
    "print(f'Using {nw} dataloader workers')\n",
    "\n",
    "# Create the data loader\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data_set,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=combine  # Custom collate function\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d33cc9d-49a7-409e-8d51-d3edb29941c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,dictii=train_data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2421298-f607-44fa-bfbe-d1cd92b2f5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.7694, 1.7694, 1.7694,  ..., 1.5982, 1.5982, 1.5982],\n",
       "         [1.7694, 1.7694, 1.7694,  ..., 1.5982, 1.5982, 1.5982],\n",
       "         [1.7694, 1.7694, 1.7694,  ..., 1.5982, 1.5982, 1.5982],\n",
       "         ...,\n",
       "         [1.4440, 1.4440, 1.4612,  ..., 1.3927, 1.3927, 1.3927],\n",
       "         [1.4440, 1.4440, 1.4612,  ..., 1.3927, 1.3927, 1.3927],\n",
       "         [1.4440, 1.4440, 1.4612,  ..., 1.3927, 1.3927, 1.3927]],\n",
       "\n",
       "        [[1.9384, 1.9384, 1.9384,  ..., 1.7633, 1.7633, 1.7633],\n",
       "         [1.9384, 1.9384, 1.9384,  ..., 1.7633, 1.7633, 1.7633],\n",
       "         [1.9384, 1.9384, 1.9384,  ..., 1.7633, 1.7633, 1.7633],\n",
       "         ...,\n",
       "         [1.6057, 1.6057, 1.6232,  ..., 1.5532, 1.5532, 1.5532],\n",
       "         [1.6057, 1.6057, 1.6232,  ..., 1.5532, 1.5532, 1.5532],\n",
       "         [1.6057, 1.6057, 1.6232,  ..., 1.5532, 1.5532, 1.5532]],\n",
       "\n",
       "        [[2.1520, 2.1520, 2.1520,  ..., 2.0125, 2.0125, 2.0125],\n",
       "         [2.1520, 2.1520, 2.1520,  ..., 2.0125, 2.0125, 2.0125],\n",
       "         [2.1520, 2.1520, 2.1520,  ..., 2.0125, 2.0125, 2.0125],\n",
       "         ...,\n",
       "         [1.7860, 1.7860, 1.8034,  ..., 1.7685, 1.7685, 1.7685],\n",
       "         [1.8034, 1.8034, 1.8208,  ..., 1.7685, 1.7685, 1.7685],\n",
       "         [1.8208, 1.8208, 1.8383,  ..., 1.7685, 1.7685, 1.7685]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f25c180f-307f-46f9-9111-a91d2323520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image=[]\n",
    "# target=[]\n",
    "# for images, targets in train_data_loader:\n",
    "#     image.append(images)\n",
    "#     target.append(targets)\n",
    "#     # process batch\n",
    "#     break  # remove this if you want all batches\n",
    "all_images = []\n",
    "all_targets= []\n",
    "\n",
    "num_batches = 1\n",
    "batch_count = 0\n",
    "\n",
    "for images, targets in train_data_loader:\n",
    "    all_images.extend(images)           # images: Tensor of shape [32, C, H, W]\n",
    "    all_targets.extend(targets)\n",
    "\n",
    "    batch_count += 1\n",
    "    if batch_count == num_batches:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9646b49d-f13b-4e47-8060-c97814cb2f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[1.7694, 1.7694, 1.7694,  ..., 1.5982, 1.5982, 1.5982],\n",
       "          [1.7694, 1.7694, 1.7694,  ..., 1.5982, 1.5982, 1.5982],\n",
       "          [1.7694, 1.7694, 1.7694,  ..., 1.5982, 1.5982, 1.5982],\n",
       "          ...,\n",
       "          [1.4440, 1.4440, 1.4612,  ..., 1.3927, 1.3927, 1.3927],\n",
       "          [1.4440, 1.4440, 1.4612,  ..., 1.3927, 1.3927, 1.3927],\n",
       "          [1.4440, 1.4440, 1.4612,  ..., 1.3927, 1.3927, 1.3927]],\n",
       " \n",
       "         [[1.9384, 1.9384, 1.9384,  ..., 1.7633, 1.7633, 1.7633],\n",
       "          [1.9384, 1.9384, 1.9384,  ..., 1.7633, 1.7633, 1.7633],\n",
       "          [1.9384, 1.9384, 1.9384,  ..., 1.7633, 1.7633, 1.7633],\n",
       "          ...,\n",
       "          [1.6057, 1.6057, 1.6232,  ..., 1.5532, 1.5532, 1.5532],\n",
       "          [1.6057, 1.6057, 1.6232,  ..., 1.5532, 1.5532, 1.5532],\n",
       "          [1.6057, 1.6057, 1.6232,  ..., 1.5532, 1.5532, 1.5532]],\n",
       " \n",
       "         [[2.1520, 2.1520, 2.1520,  ..., 2.0125, 2.0125, 2.0125],\n",
       "          [2.1520, 2.1520, 2.1520,  ..., 2.0125, 2.0125, 2.0125],\n",
       "          [2.1520, 2.1520, 2.1520,  ..., 2.0125, 2.0125, 2.0125],\n",
       "          ...,\n",
       "          [1.7860, 1.7860, 1.8034,  ..., 1.7685, 1.7685, 1.7685],\n",
       "          [1.8034, 1.8034, 1.8208,  ..., 1.7685, 1.7685, 1.7685],\n",
       "          [1.8208, 1.8208, 1.8383,  ..., 1.7685, 1.7685, 1.7685]]]),\n",
       " tensor([[[ 0.1426,  0.2111,  0.2796,  ...,  0.5707,  0.5707,  0.5536],\n",
       "          [ 0.1768,  0.2453,  0.3138,  ...,  0.5364,  0.5536,  0.5364],\n",
       "          [ 0.2111,  0.2796,  0.3652,  ...,  0.5193,  0.5022,  0.4851],\n",
       "          ...,\n",
       "          [-0.3027, -0.3027, -0.2856,  ...,  0.3138,  0.2796,  0.2624],\n",
       "          [-0.3027, -0.3027, -0.2856,  ...,  0.2796,  0.2453,  0.2282],\n",
       "          [-0.3198, -0.3027, -0.2856,  ...,  0.2111,  0.1939,  0.1939]],\n",
       " \n",
       "         [[ 0.4328,  0.5028,  0.5903,  ...,  0.8529,  0.8179,  0.8004],\n",
       "          [ 0.4678,  0.5378,  0.6254,  ...,  0.8179,  0.8004,  0.7829],\n",
       "          [ 0.5028,  0.5903,  0.6779,  ...,  0.8004,  0.7829,  0.7654],\n",
       "          ...,\n",
       "          [ 0.0826,  0.0826,  0.1001,  ...,  0.6779,  0.6779,  0.6604],\n",
       "          [ 0.0826,  0.0826,  0.1001,  ...,  0.6429,  0.6429,  0.6254],\n",
       "          [ 0.0651,  0.0826,  0.1001,  ...,  0.5728,  0.5903,  0.5903]],\n",
       " \n",
       "         [[ 1.1759,  1.2282,  1.2805,  ...,  1.4374,  1.4200,  1.4025],\n",
       "          [ 1.1934,  1.2457,  1.3154,  ...,  1.4025,  1.4025,  1.3851],\n",
       "          [ 1.2282,  1.2805,  1.3328,  ...,  1.3851,  1.3677,  1.3502],\n",
       "          ...,\n",
       "          [ 0.8448,  0.8448,  0.8622,  ...,  1.3154,  1.2980,  1.2805],\n",
       "          [ 0.8448,  0.8448,  0.8622,  ...,  1.2631,  1.2631,  1.2457],\n",
       "          [ 0.8274,  0.8448,  0.8622,  ...,  1.2108,  1.2108,  1.2108]]]),\n",
       " tensor([[[-0.9534, -0.9534, -0.9534,  ..., -0.9020, -0.9020, -0.9020],\n",
       "          [-0.9534, -0.9534, -0.9534,  ..., -0.9020, -0.9020, -0.9020],\n",
       "          [-0.9534, -0.9534, -0.9534,  ..., -0.9020, -0.9020, -0.9020],\n",
       "          ...,\n",
       "          [ 1.4612,  1.4783,  1.5125,  ...,  1.6838,  1.7865,  1.8379],\n",
       "          [ 1.3927,  1.4612,  1.4612,  ...,  1.7180,  1.6838,  1.6153],\n",
       "          [ 1.7009,  1.7180,  1.3927,  ...,  1.7523,  1.5468,  1.3584]],\n",
       " \n",
       "         [[ 0.2752,  0.2752,  0.2752,  ...,  0.4153,  0.4153,  0.4153],\n",
       "          [ 0.2752,  0.2752,  0.2752,  ...,  0.4153,  0.4153,  0.4153],\n",
       "          [ 0.2752,  0.2752,  0.2752,  ...,  0.4153,  0.4153,  0.4153],\n",
       "          ...,\n",
       "          [ 1.7108,  1.7283,  1.7633,  ...,  1.7808,  1.8683,  1.9209],\n",
       "          [ 1.6408,  1.7108,  1.7108,  ...,  1.8333,  1.7983,  1.7283],\n",
       "          [ 1.9559,  1.9734,  1.6408,  ...,  1.8859,  1.6758,  1.4832]],\n",
       " \n",
       "         [[ 1.7337,  1.7337,  1.7337,  ...,  1.7685,  1.7685,  1.7685],\n",
       "          [ 1.7337,  1.7337,  1.7337,  ...,  1.7685,  1.7685,  1.7685],\n",
       "          [ 1.7337,  1.7337,  1.7337,  ...,  1.7685,  1.7685,  1.7685],\n",
       "          ...,\n",
       "          [ 1.8383,  1.8557,  1.9254,  ...,  1.6640,  1.7511,  1.8034],\n",
       "          [ 1.7511,  1.8208,  1.8208,  ...,  1.7511,  1.7337,  1.6640],\n",
       "          [ 2.0648,  2.0648,  1.7511,  ...,  1.8557,  1.6291,  1.4548]]]),\n",
       " tensor([[[-0.9192, -0.9192, -0.9192,  ..., -0.9877, -0.9877, -0.9877],\n",
       "          [-0.9192, -0.9192, -0.9192,  ..., -0.9877, -0.9877, -0.9877],\n",
       "          [-0.9192, -0.9192, -0.9192,  ..., -0.9877, -0.9877, -0.9877],\n",
       "          ...,\n",
       "          [-0.7993, -0.7822, -0.7650,  ..., -0.8335, -0.8335, -0.8335],\n",
       "          [-0.7993, -0.7822, -0.7650,  ..., -0.8335, -0.8335, -0.8335],\n",
       "          [-0.7993, -0.7822, -0.7650,  ..., -0.8335, -0.8335, -0.8335]],\n",
       " \n",
       "         [[-0.0049, -0.0049, -0.0049,  ..., -0.0924, -0.0924, -0.0924],\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0924, -0.0924, -0.0924],\n",
       "          [-0.0049, -0.0049, -0.0049,  ..., -0.0924, -0.0924, -0.0924],\n",
       "          ...,\n",
       "          [ 0.1702,  0.1877,  0.2052,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          [ 0.1702,  0.1877,  0.2052,  ...,  0.0651,  0.0651,  0.0651],\n",
       "          [ 0.1702,  0.1877,  0.2052,  ...,  0.0651,  0.0651,  0.0651]],\n",
       " \n",
       "         [[ 1.2631,  1.2631,  1.2631,  ...,  1.1237,  1.1237,  1.1237],\n",
       "          [ 1.2631,  1.2631,  1.2631,  ...,  1.1237,  1.1237,  1.1237],\n",
       "          [ 1.2631,  1.2631,  1.2631,  ...,  1.1237,  1.1237,  1.1237],\n",
       "          ...,\n",
       "          [ 1.4025,  1.4200,  1.4374,  ...,  1.2805,  1.2805,  1.2805],\n",
       "          [ 1.4025,  1.4200,  1.4374,  ...,  1.2805,  1.2805,  1.2805],\n",
       "          [ 1.4025,  1.4200,  1.4374,  ...,  1.2805,  1.2805,  1.2805]]]),\n",
       " tensor([[[ 1.9578,  1.9578,  1.9578,  ...,  1.8722,  1.8550,  1.8550],\n",
       "          [ 1.9578,  1.9578,  1.9578,  ...,  1.8722,  1.8550,  1.8379],\n",
       "          [ 1.9578,  1.9578,  1.9578,  ...,  1.8550,  1.8379,  1.8208],\n",
       "          ...,\n",
       "          [-0.5767,  0.9646,  1.7523,  ...,  0.9646,  0.9817,  0.9988],\n",
       "          [-0.7308, -0.1828, -0.1314,  ...,  1.2043,  1.1700,  1.1872],\n",
       "          [-0.7308, -0.7308, -0.3541,  ...,  1.2043,  1.1358,  1.1358]],\n",
       " \n",
       "         [[ 2.1310,  2.1310,  2.1310,  ...,  2.0434,  2.0609,  2.0609],\n",
       "          [ 2.1310,  2.1310,  2.1310,  ...,  2.0434,  2.0434,  2.0434],\n",
       "          [ 2.1310,  2.1310,  2.1310,  ...,  2.0259,  2.0259,  2.0259],\n",
       "          ...,\n",
       "          [-0.6001,  0.8880,  1.5007,  ...,  1.1155,  1.1506,  1.1681],\n",
       "          [-0.8277, -0.3550, -0.4601,  ...,  1.3782,  1.3431,  1.3606],\n",
       "          [-0.8277, -0.9328, -0.6877,  ...,  1.3782,  1.3081,  1.3081]],\n",
       " \n",
       "         [[ 2.3437,  2.3437,  2.3437,  ...,  2.2566,  2.2566,  2.2566],\n",
       "          [ 2.3437,  2.3437,  2.3437,  ...,  2.2566,  2.2391,  2.2391],\n",
       "          [ 2.3437,  2.3437,  2.3437,  ...,  2.2391,  2.2217,  2.2217],\n",
       "          ...,\n",
       "          [-0.3404,  1.1062,  1.4722,  ...,  1.3502,  1.3328,  1.2980],\n",
       "          [-0.4973, -0.1487, -0.3404,  ...,  1.6117,  1.5420,  1.4897],\n",
       "          [-0.5147, -0.7413, -0.6367,  ...,  1.6117,  1.4897,  1.4374]]]),\n",
       " tensor([[[0.9646, 1.0159, 0.9817,  ..., 0.8618, 0.9132, 0.9132],\n",
       "          [1.1187, 1.1700, 1.1358,  ..., 1.0159, 1.0673, 1.0673],\n",
       "          [1.0502, 1.1015, 1.0502,  ..., 0.9474, 0.9646, 0.9646],\n",
       "          ...,\n",
       "          [0.2624, 0.1768, 0.2282,  ..., 0.4166, 0.3994, 0.3994],\n",
       "          [0.0398, 0.1597, 0.1768,  ..., 0.3823, 0.3481, 0.3652],\n",
       "          [0.0056, 0.1254, 0.1254,  ..., 0.3994, 0.3652, 0.3823]],\n",
       " \n",
       "         [[1.3431, 1.3957, 1.3606,  ..., 1.2381, 1.2906, 1.2906],\n",
       "          [1.5007, 1.5532, 1.5182,  ..., 1.3957, 1.4482, 1.4482],\n",
       "          [1.4307, 1.4832, 1.4307,  ..., 1.3256, 1.3431, 1.3431],\n",
       "          ...,\n",
       "          [0.3452, 0.2577, 0.2927,  ..., 0.4503, 0.4153, 0.4153],\n",
       "          [0.1877, 0.3102, 0.2752,  ..., 0.3803, 0.3452, 0.3627],\n",
       "          [0.1702, 0.2752, 0.2402,  ..., 0.3803, 0.3452, 0.3627]],\n",
       " \n",
       "         [[1.7163, 1.7685, 1.7337,  ..., 1.6117, 1.6640, 1.6640],\n",
       "          [1.8731, 1.9254, 1.8905,  ..., 1.7685, 1.8208, 1.8208],\n",
       "          [1.8034, 1.8557, 1.8034,  ..., 1.6988, 1.7163, 1.7163],\n",
       "          ...,\n",
       "          [0.3916, 0.3045, 0.3568,  ..., 0.4788, 0.5136, 0.5311],\n",
       "          [0.1999, 0.3219, 0.3045,  ..., 0.4265, 0.4439, 0.4962],\n",
       "          [0.1651, 0.2871, 0.2696,  ..., 0.4265, 0.4439, 0.4962]]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06591c79-82e7-4901-a608-fe79632678f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "\n",
    "def denormalize_image(img_tensor, mean, std):\n",
    "    \"\"\"Denormalize image tensor.\"\"\"\n",
    "    img_tensor = img_tensor.clone()  # avoid modifying original\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img_tensor.clamp(0, 1)\n",
    "dict = builtins.dict\n",
    "def plot_images_from_loader(data_loader, class_names, num_images=10, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        for img, target in zip(images, targets):\n",
    "            if images_shown >= num_images:\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                return\n",
    "\n",
    "            # Denormalize and convert to PIL\n",
    "            img = denormalize_image(img, mean, std)\n",
    "            img_pil = F.to_pil_image(img)\n",
    "\n",
    "            plt.subplot(5, 2, images_shown + 1)\n",
    "            plt.imshow(img_pil)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Image ID: {target['image_id'].item()}\")\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            boxes = target[\"boxes\"]\n",
    "            labels = target[\"labels\"]\n",
    "            for box, label in zip(boxes, labels):\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                plt.gca().add_patch(plt.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    fill=False, edgecolor='red', linewidth=2\n",
    "                ))\n",
    "                plt.gca().text(\n",
    "                    x1, max(y1 - 5, 0), class_names[label],\n",
    "                    fontsize=10, color='white',\n",
    "                    bbox=dict(facecolor='red', alpha=0.5)  # use original 'dict'\n",
    "                )\n",
    "\n",
    "            images_shown += 1\n",
    "        if images_shown >= num_images:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dfad658-eaf9-465c-b96b-725d678a1acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# class_names = list(train_data_set.classes)  # ['__background__', 'UAV']\n",
    "# plot_images_from_loader(train_data_loader, class_names, num_images=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb9d90-f799-4cde-aa6c-93ac9f8a7e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177e8a3-0def-4824-81fa-5f83486d2f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f66bb-7b06-437a-ae16-3265c7e59fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 2.2.2\n",
      "Using cuda device training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/862]  eta: 2 days, 2:29:26.386721  lr: 0.005000  loss: 6.1905 (6.1905)  loss_classifier: 4.2763 (4.2763)  loss_box_reg: 0.0005 (0.0005)  loss_objectness: 0.6923 (0.6923)  loss_rpn_box_reg: 1.2214 (1.2214)  time: 210.8659  data: 0.0327  max mem: 5531\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "# from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F   \n",
    "# from config.train_config import cfg\n",
    "# from dataloader.coco_dataset import coco\n",
    "# from utils.evaluate_utils import evaluate\n",
    "# from utils.im_utils import Compose, ToTensor, RandomHorizontalFlip\n",
    "# from utils.plot_utils import plot_loss_and_lr, plot_map\n",
    "# from utils.train_utils import train_one_epoch, write_tb, create_model\n",
    "# import utils.boxes_utils as box_op\n",
    "\n",
    "def main():\n",
    "    device = torch.device(cfg.device_name)\n",
    "    print(\"Using {} device training.\".format(device.type))\n",
    "\n",
    "    # if not os.path.exists(cfg.model_save_dir):\n",
    "    #     os.makedirs(cfg.model_save_dir)\n",
    "\n",
    "    # tensorboard writer\n",
    "    # writer = SummaryWriter(os.path.join(cfg.model_save_dir, 'epoch_log'))\n",
    "\n",
    "    # data_transform = {\n",
    "    #     \"train\": Compose([ToTensor(), RandomHorizontalFlip(cfg.train_horizon_flip_prob)]),\n",
    "    #     \"val\": Compose([ToTensor()])\n",
    "    # }\n",
    "\n",
    "    # if not os.path.exists(cfg.data_root_dir):\n",
    "    #     raise FileNotFoundError(\"dataset root dir not exist!\")\n",
    "\n",
    "    # load train data set\n",
    "    # train_data_set = coco(cfg.data_root_dir, 'train', '2017', data_transform[\"train\"])\n",
    "    # batch_size = cfg.batch_size\n",
    "    # nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    # print('Using {} dataloader workers'.format(nw))\n",
    "    # train_data_loader = torch.utils.data.DataLoader(train_data_set,\n",
    "    #                                                 batch_size=batch_size,\n",
    "    #                                                 shuffle=True,\n",
    "    #                                                 num_workers=nw,\n",
    "    #                                                 collate_fn=train_data_set.collate_fn)\n",
    "\n",
    "    # # load validation data set\n",
    "    # val_data_set = coco(cfg.data_root_dir, 'val', '2017', data_transform[\"val\"])\n",
    "    # val_data_set_loader = torch.utils.data.DataLoader(val_data_set,\n",
    "    #                                                   batch_size=batch_size,\n",
    "    #                                                   shuffle=False,\n",
    "    #                                                   num_workers=nw,\n",
    "    #                                                   collate_fn=train_data_set.collate_fn)\n",
    "\n",
    "    # create model num_classes equal background + 80 classes\n",
    "    model = create_model(num_classes=cfg.num_class)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # define optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=cfg.lr,\n",
    "                                momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=cfg.lr_dec_step_size,\n",
    "                                                   gamma=cfg.lr_gamma)\n",
    "\n",
    "    # train from pretrained weights\n",
    "    if cfg.resume != \"\":\n",
    "        checkpoint = torch.load(cfg.resume)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        cfg.start_epoch = checkpoint['epoch'] + 1\n",
    "        print(\"the training process from epoch{}...\".format(cfg.start_epoch))\n",
    "\n",
    "    train_loss = []\n",
    "    learning_rate = []\n",
    "    train_mAP_list = []\n",
    "    val_mAP = []\n",
    "\n",
    "    best_mAP = 0\n",
    "    for epoch in range(cfg.start_epoch, cfg.num_epochs):\n",
    "        loss_dict, total_loss = train_one_epoch(model, optimizer, train_data_loader,\n",
    "                                                device, epoch, train_loss=train_loss, train_lr=learning_rate,\n",
    "                                                print_freq=50, warmup=False)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(\"------>Starting training data valid\")\n",
    "        _, train_mAP = evaluate(model, train_data_loader, device=device, mAP_list=train_mAP_list)\n",
    "\n",
    "        # print(\"------>Starting validation data valid\")\n",
    "        # _, mAP = evaluate(model, val_data_set_loader, device=device, mAP_list=val_mAP)\n",
    "        print('training mAp is {}'.format(train_mAP))\n",
    "        print('total loss is {}'.format(total_loss))\n",
    "        # print('validation mAp is {}'.format(mAP))\n",
    "        print('best mAp is {}'.format(best_mAP))\n",
    "\n",
    "        # board_info = {'lr': optimizer.param_groups[0]['lr'],\n",
    "        #               'train_mAP': train_mAP}\n",
    "\n",
    "        # for k, v in loss_dict.items():\n",
    "        #     board_info[k] = v.item()\n",
    "        # board_info['total loss'] = total_loss.item()\n",
    "        # write_tb(writer, epoch, board_info)\n",
    "\n",
    "        if mAP > best_mAP:\n",
    "            best_mAP = mAP\n",
    "            # save weights\n",
    "            save_files = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch}\n",
    "            model_save_dir ='Best_model_FRCNN'\n",
    "            torch.save(save_files,\n",
    "                       os.path.join(model_save_dir, \"{}-model-{}-mAp-{}.pth\".format(cfg.backbone, epoch, mAP)))\n",
    "    # writer.close()\n",
    "    # plot loss and lr curve\n",
    "    if len(train_loss) != 0 and len(learning_rate) != 0:\n",
    "        plot_loss_and_lr(train_loss, learning_rate, cfg.model_save_dir)\n",
    "\n",
    "    # plot mAP curve\n",
    "    if len(val_mAP) != 0:\n",
    "        plot_map(val_mAP, cfg.model_save_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    version = torch.version.__version__[:5]\n",
    "    print('torch version is {}'.format(version))\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b6743-5585-455b-93bb-54faa247eac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6f529-7c30-427a-b627-b800a69c0735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf67878-c784-4c47-903c-f1ff8b38423e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824132bc-2b41-4679-b609-bdf425ea75d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90568eaf-f0f1-48c8-a9d0-5bfaa082baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
