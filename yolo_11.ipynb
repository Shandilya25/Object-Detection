{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709a155-a6f4-4789-811f-be529c1f9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "def setup_seed():\n",
    "    \"\"\"\n",
    "    Setup random seed.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    numpy.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def setup_multi_processes():\n",
    "    \"\"\"\n",
    "    Setup multi-processing environment variables.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    from os import environ\n",
    "    from platform import system\n",
    "\n",
    "    # set multiprocess start method as `fork` to speed up the training\n",
    "    if system() != 'Windows':\n",
    "        torch.multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    # disable opencv multithreading to avoid system being overloaded\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # setup OMP threads\n",
    "    if 'OMP_NUM_THREADS' not in environ:\n",
    "        environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    # setup MKL threads\n",
    "    if 'MKL_NUM_THREADS' not in environ:\n",
    "        environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "def export_onnx(args):\n",
    "    import onnx  # noqa\n",
    "\n",
    "    inputs = ['images']\n",
    "    outputs = ['outputs']\n",
    "    dynamic = {'outputs': {0: 'batch', 1: 'anchors'}}\n",
    "\n",
    "    m = torch.load('./weights/best.pt')['model'].float()\n",
    "    x = torch.zeros((1, 3, args.input_size, args.input_size))\n",
    "\n",
    "    torch.onnx.export(m.cpu(), x.cpu(),\n",
    "                      f='./weights/best.onnx',\n",
    "                      verbose=False,\n",
    "                      opset_version=12,\n",
    "                      # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False\n",
    "                      do_constant_folding=True,\n",
    "                      input_names=inputs,\n",
    "                      output_names=outputs,\n",
    "                      dynamic_axes=dynamic or None)\n",
    "    model_onnx = onnx.load('./weights/best.onnx')  # load onnx model\n",
    "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "\n",
    "    onnx.save(model_onnx, './weights/best.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e14b9b-2cc5-4560-bc9c-389154107c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa88124-4839-4569-81cb-6a22fb78c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wh2xy(x):\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else numpy.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_anchors(x, strides, offset=0.5):\n",
    "    assert x is not None\n",
    "    anchor_tensor, stride_tensor = [], []\n",
    "    dtype, device = x[0].dtype, x[0].device\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # shift x\n",
    "        sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "    return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "def compute_metric(output, target, iou_v):\n",
    "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2) = target[:, 1:].unsqueeze(1).chunk(2, 2)\n",
    "    (b1, b2) = output[:, :4].unsqueeze(0).chunk(2, 2)\n",
    "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "    # IoU = intersection / (area1 + area2 - intersection)\n",
    "    iou = intersection / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - intersection + 1e-7)\n",
    "\n",
    "    correct = numpy.zeros((output.shape[0], iou_v.shape[0]))\n",
    "    correct = correct.astype(bool)\n",
    "    for i in range(len(iou_v)):\n",
    "        # IoU > threshold and classes match\n",
    "        x = torch.where((iou >= iou_v[i]) & (target[:, 0:1] == output[:, 5]))\n",
    "        if x[0].shape[0]:\n",
    "            matches = torch.cat((torch.stack(x, 1),\n",
    "                                 iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
    "            if x[0].shape[0] > 1:\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
    "                matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
    "            correct[matches[:, 1].astype(int), i] = True\n",
    "    return torch.tensor(correct, dtype=torch.bool, device=output.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd161c-5735-4cc1-a742-1efd98f3ced2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5806ddb-2205-4d74-858d-987d714dfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(outputs, confidence_threshold=0.001, iou_threshold=0.65):\n",
    "    max_wh = 7680\n",
    "    max_det = 300\n",
    "    max_nms = 30000\n",
    "\n",
    "    bs = outputs.shape[0]  # batch size\n",
    "    nc = outputs.shape[1] - 4  # number of classes\n",
    "    xc = outputs[:, 4:4 + nc].amax(1) > confidence_threshold  # candidates\n",
    "\n",
    "    # Settings\n",
    "    start = time()\n",
    "    limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    output = [torch.zeros((0, 6), device=outputs.device)] * bs\n",
    "    for index, x in enumerate(outputs):  # image index, image inference\n",
    "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # matrix nx6 (box, confidence, cls)\n",
    "        box, cls = x.split((4, nc), 1)\n",
    "        box = wh2xy(box)  # (cx, cy, w, h) to (x1, y1, x2, y2)\n",
    "        if nc > 1:\n",
    "            i, j = (cls > confidence_threshold).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > confidence_threshold]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes, scores\n",
    "        indices = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
    "        indices = indices[:max_det]  # limit detections\n",
    "\n",
    "        output[index] = x[indices]\n",
    "        if (time() - start) > limit:\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447a465-30fa-41f2-bbf5-272a22a4f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, f=0.1):\n",
    "    # Box filter of fraction f\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = numpy.ones(nf // 2)  # ones padding\n",
    "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
    "\n",
    "\n",
    "def plot_pr_curve(px, py, ap, names, save_dir):\n",
    "    from matplotlib import pyplot\n",
    "    fig, ax = pyplot.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n",
    "    py = numpy.stack(py, axis=1)\n",
    "\n",
    "    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n",
    "        for i, y in enumerate(py.T):\n",
    "            ax.plot(px, y, linewidth=1, label=f\"{names[i]} {ap[i, 0]:.3f}\")  # plot(recall, precision)\n",
    "    else:\n",
    "        ax.plot(px, py, linewidth=1, color=\"grey\")  # plot(recall, precision)\n",
    "\n",
    "    ax.plot(px, py.mean(1), linewidth=3, color=\"blue\", label=\"all classes %.3f mAP@0.5\" % ap[:, 0].mean())\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    ax.set_title(\"Precision-Recall Curve\")\n",
    "    fig.savefig(save_dir, dpi=250)\n",
    "    pyplot.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae5c43-a625-4979-9af7-513890db32fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9054b3-62b7-4530-8fdd-29774f26c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(px, py, names, save_dir, x_label=\"Confidence\", y_label=\"Metric\"):\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    figure, ax = pyplot.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n",
    "\n",
    "    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n",
    "        for i, y in enumerate(py):\n",
    "            ax.plot(px, y, linewidth=1, label=f\"{names[i]}\")  # plot(confidence, metric)\n",
    "    else:\n",
    "        ax.plot(px, py.T, linewidth=1, color=\"grey\")  # plot(confidence, metric)\n",
    "\n",
    "    y = smooth(py.mean(0), f=0.05)\n",
    "    ax.plot(px, y, linewidth=3, color=\"blue\", label=f\"all classes {y.max():.3f} at {px[y.argmax()]:.3f}\")\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    ax.set_title(f\"{y_label}-Confidence Curve\")\n",
    "    figure.savefig(save_dir, dpi=250)\n",
    "    pyplot.close(figure)\n",
    "\n",
    "\n",
    "def compute_ap(tp, conf, output, target, plot=False, names=(), eps=1E-16):\n",
    "    \"\"\"\n",
    "    Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Object-ness value from 0-1 (nparray).\n",
    "        output:  Predicted object classes (nparray).\n",
    "        target:  True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision\n",
    "    \"\"\"\n",
    "    # Sort by object-ness\n",
    "    i = numpy.argsort(-conf)\n",
    "    tp, conf, output = tp[i], conf[i], output[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = numpy.unique(target, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    p = numpy.zeros((nc, 1000))\n",
    "    r = numpy.zeros((nc, 1000))\n",
    "    ap = numpy.zeros((nc, tp.shape[1]))\n",
    "    px, py = numpy.linspace(start=0, stop=1, num=1000), []  # for plotting\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = output == c\n",
    "        nl = nt[ci]  # number of labels\n",
    "        no = i.sum()  # number of outputs\n",
    "        if no == 0 or nl == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (nl + eps)  # recall curve\n",
    "        # negative x, xp because xp decreases\n",
    "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
    "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
    "\n",
    "            # Compute the precision envelope\n",
    "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
    "\n",
    "            # Integrate area under curve\n",
    "            x = numpy.linspace(start=0, stop=1, num=101)  # 101-point interp (COCO)\n",
    "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
    "            if plot and j == 0:\n",
    "                py.append(numpy.interp(px, m_rec, m_pre))  # precision at mAP@0.5\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "    if plot:\n",
    "        names = dict(enumerate(names))  # to dict\n",
    "        names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "        plot_pr_curve(px, py, ap, names, save_dir=\"./weights/PR_curve.png\")\n",
    "        plot_curve(px, f1, names, save_dir=\"./weights/F1_curve.png\", y_label=\"F1\")\n",
    "        plot_curve(px, p, names, save_dir=\"./weights/P_curve.png\", y_label=\"Precision\")\n",
    "        plot_curve(px, r, names, save_dir=\"./weights/R_curve.png\", y_label=\"Recall\")\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    m_pre, m_rec = p.mean(), r.mean()\n",
    "    map50, mean_ap = ap50.mean(), ap.mean()\n",
    "    return tp, fp, m_pre, m_rec, map50, mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638236f-52fb-4ae1-8e0e-753d2f7126b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef7e41-e56f-45bc-8ba8-ae6308fa36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2, eps=1e-7):\n",
    "    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp(0) * \\\n",
    "            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    # IoU\n",
    "    iou = inter / union\n",
    "    cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n",
    "    ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "    c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "    rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n",
    "    # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "    v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "    with torch.no_grad():\n",
    "        alpha = v / (v - iou + (1 + eps))\n",
    "    return iou - (rho2 / c2 + v * alpha)  # CIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda655e-ff85-414a-92ff-f912cf3e6183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87012e59-ac67-4a6c-a237-f7a48fd57715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def strip_optimizer(filename):\n",
    "    x = torch.load(filename, map_location=\"cpu\")\n",
    "    x['model'].half()  # to FP16\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    torch.save(x, f=filename)\n",
    "\n",
    "\n",
    "def clip_gradients(model, max_norm=10.0):\n",
    "    parameters = model.parameters()\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
    "\n",
    "\n",
    "def load_weight(model, ckpt):\n",
    "    dst = model.state_dict()\n",
    "    src = torch.load(ckpt)['model'].float().cpu()\n",
    "\n",
    "    ckpt = {}\n",
    "    for k, v in src.state_dict().items():\n",
    "        if k in dst and v.shape == dst[k].shape:\n",
    "            ckpt[k] = v\n",
    "\n",
    "    model.load_state_dict(state_dict=ckpt, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda99c1-bc92-46a5-afb9-f5448fbe9178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28121b66-589d-4f3b-acd7-d677e9feaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(model, decay):\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    norm = tuple(v for k, v in torch.nn.__dict__.items() if \"Norm\" in k)\n",
    "    for m in model.modules():\n",
    "        for n, p in m.named_parameters(recurse=0):\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if n == \"bias\":  # bias (no decay)\n",
    "                p1.append(p)\n",
    "            elif n == \"weight\" and isinstance(m, norm):  # norm-weight (no decay)\n",
    "                p1.append(p)\n",
    "            else:\n",
    "                p2.append(p)  # weight (with decay)\n",
    "    return [{'params': p1, 'weight_decay': 0.00},\n",
    "            {'params': p2, 'weight_decay': decay}]\n",
    "\n",
    "\n",
    "def plot_lr(args, optimizer, scheduler, num_steps):\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    optimizer = copy.copy(optimizer)\n",
    "    scheduler = copy.copy(scheduler)\n",
    "\n",
    "    y = []\n",
    "    for epoch in range(args.epochs):\n",
    "        for i in range(num_steps):\n",
    "            step = i + num_steps * epoch\n",
    "            scheduler.step(step, optimizer)\n",
    "            y.append(optimizer.param_groups[0]['lr'])\n",
    "    pyplot.plot(y, '.-', label='LR')\n",
    "    pyplot.xlabel('step')\n",
    "    pyplot.ylabel('LR')\n",
    "    pyplot.grid()\n",
    "    pyplot.xlim(0, args.epochs * num_steps)\n",
    "    pyplot.ylim(0)\n",
    "    pyplot.savefig('./weights/lr.png', dpi=200)\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "class CosineLR:\n",
    "    def __init__(self, args, params, num_steps):\n",
    "        max_lr = params['max_lr']\n",
    "        min_lr = params['min_lr']\n",
    "\n",
    "        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n",
    "        decay_steps = int(args.epochs * num_steps - warmup_steps)\n",
    "\n",
    "        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps))\n",
    "\n",
    "        decay_lr = []\n",
    "        for step in range(1, decay_steps + 1):\n",
    "            alpha = math.cos(math.pi * step / decay_steps)\n",
    "            decay_lr.append(min_lr + 0.5 * (max_lr - min_lr) * (1 + alpha))\n",
    "\n",
    "        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n",
    "\n",
    "    def step(self, step, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = self.total_lr[step]\n",
    "\n",
    "\n",
    "class LinearLR:\n",
    "    def __init__(self, args, params, num_steps):\n",
    "        max_lr = params['max_lr']\n",
    "        min_lr = params['min_lr']\n",
    "\n",
    "        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n",
    "        decay_steps = int(args.epochs * num_steps - warmup_steps)\n",
    "\n",
    "        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps), endpoint=False)\n",
    "        decay_lr = numpy.linspace(max_lr, min_lr, decay_steps)\n",
    "\n",
    "        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n",
    "\n",
    "    def step(self, step, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = self.total_lr[step]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342865a5-5f5e-4b6c-9ed3-2fdf87106b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab968e4d-21cd-4f3d-9528-9c258aa24799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMA:\n",
    "    \"\"\"\n",
    "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
    "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
    "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
    "        # Create EMA\n",
    "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
    "        self.updates = updates  # number of EMA updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "\n",
    "            msd = model.state_dict()  # model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1 - d) * msd[k].detach()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, v, n):\n",
    "        if not math.isnan(float(v)):\n",
    "            self.num = self.num + n\n",
    "            self.sum = self.sum + v * n\n",
    "            self.avg = self.sum / self.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fce59e-1dca-43e6-a211-8afa16936b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc078c-bf7c-4a97-8a94-6957d7d466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assigner(torch.nn.Module):\n",
    "    def __init__(self, nc=80, top_k=13, alpha=1.0, beta=6.0, eps=1E-9):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.nc = nc\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
    "        batch_size = pd_scores.size(0)\n",
    "        num_max_boxes = gt_bboxes.size(1)\n",
    "\n",
    "        if num_max_boxes == 0:\n",
    "            device = gt_bboxes.device\n",
    "            return (torch.zeros_like(pd_bboxes).to(device),\n",
    "                    torch.zeros_like(pd_scores).to(device),\n",
    "                    torch.zeros_like(pd_scores[..., 0]).to(device))\n",
    "\n",
    "        num_anchors = anc_points.shape[0]\n",
    "        shape = gt_bboxes.shape\n",
    "        lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)\n",
    "        mask_in_gts = torch.cat((anc_points[None] - lt, rb - anc_points[None]), dim=2)\n",
    "        mask_in_gts = mask_in_gts.view(shape[0], shape[1], num_anchors, -1).amin(3).gt_(self.eps)\n",
    "        na = pd_bboxes.shape[-2]\n",
    "        gt_mask = (mask_in_gts * mask_gt).bool()  # b, max_num_obj, h*w\n",
    "        overlaps = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\n",
    "        bbox_scores = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\n",
    "\n",
    "        ind = torch.zeros([2, batch_size, num_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\n",
    "        ind[0] = torch.arange(end=batch_size).view(-1, 1).expand(-1, num_max_boxes)  # b, max_num_obj\n",
    "        ind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n",
    "        bbox_scores[gt_mask] = pd_scores[ind[0], :, ind[1]][gt_mask]  # b, max_num_obj, h*w\n",
    "\n",
    "        pd_boxes = pd_bboxes.unsqueeze(1).expand(-1, num_max_boxes, -1, -1)[gt_mask]\n",
    "        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[gt_mask]\n",
    "        overlaps[gt_mask] = compute_iou(gt_boxes, pd_boxes).squeeze(-1).clamp_(0)\n",
    "\n",
    "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
    "\n",
    "        top_k_mask = mask_gt.expand(-1, -1, self.top_k).bool()\n",
    "        top_k_metrics, top_k_indices = torch.topk(align_metric, self.top_k, dim=-1, largest=True)\n",
    "        if top_k_mask is None:\n",
    "            top_k_mask = (top_k_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(top_k_indices)\n",
    "        top_k_indices.masked_fill_(~top_k_mask, 0)\n",
    "\n",
    "        mask_top_k = torch.zeros(align_metric.shape, dtype=torch.int8, device=top_k_indices.device)\n",
    "        ones = torch.ones_like(top_k_indices[:, :, :1], dtype=torch.int8, device=top_k_indices.device)\n",
    "        for k in range(self.top_k):\n",
    "            mask_top_k.scatter_add_(-1, top_k_indices[:, :, k:k + 1], ones)\n",
    "        mask_top_k.masked_fill_(mask_top_k > 1, 0)\n",
    "        mask_top_k = mask_top_k.to(align_metric.dtype)\n",
    "        mask_pos = mask_top_k * mask_in_gts * mask_gt\n",
    "\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, num_max_boxes, -1)\n",
    "            max_overlaps_idx = overlaps.argmax(1)\n",
    "\n",
    "            is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n",
    "            is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\n",
    "\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        target_gt_idx = mask_pos.argmax(-2)\n",
    "\n",
    "        # Assigned target\n",
    "        index = torch.arange(end=batch_size, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
    "        target_index = target_gt_idx + index * num_max_boxes\n",
    "        target_labels = gt_labels.long().flatten()[target_index]\n",
    "\n",
    "        target_bboxes = gt_bboxes.view(-1, gt_bboxes.shape[-1])[target_index]\n",
    "\n",
    "        # Assigned target scores\n",
    "        target_labels.clamp_(0)\n",
    "\n",
    "        target_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.nc),\n",
    "                                    dtype=torch.int64,\n",
    "                                    device=target_labels.device)\n",
    "        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n",
    "\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "\n",
    "        # Normalize\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "\n",
    "        return target_bboxes, target_scores, fg_mask.bool()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d73159-3cce-4022-b622-3e6589c0c820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103da617-5a6d-450b-a4b7-99d9752db8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFL(torch.nn.Module):\n",
    "    def __init__(self, beta=2.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = self.bce_loss(outputs, targets)\n",
    "        return torch.pow(torch.abs(targets - outputs.sigmoid()), self.beta) * bce_loss\n",
    "\n",
    "\n",
    "class VFL(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.00, iou_weighted=True):\n",
    "        super().__init__()\n",
    "        assert alpha >= 0.0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.iou_weighted = iou_weighted\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        assert outputs.size() == targets.size()\n",
    "        targets = targets.type_as(outputs)\n",
    "\n",
    "        if self.iou_weighted:\n",
    "            focal_weight = targets * (targets > 0.0).float() + \\\n",
    "                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n",
    "                           (targets <= 0.0).float()\n",
    "\n",
    "        else:\n",
    "            focal_weight = (targets > 0.0).float() + \\\n",
    "                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n",
    "                           (targets <= 0.0).float()\n",
    "\n",
    "        return self.bce_loss(outputs, targets) * focal_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54595efa-8f41-4248-bebd-258911e1aefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405b7c5-a410-4bd5-ae5a-49cd9b04d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.bce_loss(outputs, targets)\n",
    "\n",
    "        if self.alpha > 0:\n",
    "            alpha_factor = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            loss *= alpha_factor\n",
    "\n",
    "        if self.gamma > 0:\n",
    "            outputs_sigmoid = outputs.sigmoid()\n",
    "            p_t = targets * outputs_sigmoid + (1 - targets) * (1 - outputs_sigmoid)\n",
    "            gamma_factor = (1.0 - p_t) ** self.gamma\n",
    "            loss *= gamma_factor\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612007ff-5b1c-4bf7-9597-fc333eb29582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed6266-4930-4501-acd7-d608302acd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BoxLoss(torch.nn.Module):\n",
    "    def __init__(self, dfl_ch):\n",
    "        super().__init__()\n",
    "        self.dfl_ch = dfl_ch\n",
    "\n",
    "    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
    "        # IoU loss\n",
    "        weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "        iou = compute_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
    "        loss_box = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "\n",
    "        # DFL loss\n",
    "        a, b = target_bboxes.chunk(2, -1)\n",
    "        target = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
    "        target = target.clamp(0, self.dfl_ch - 0.01)\n",
    "        loss_dfl = self.df_loss(pred_dist[fg_mask].view(-1, self.dfl_ch + 1), target[fg_mask])\n",
    "        loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
    "\n",
    "        return loss_box, loss_dfl\n",
    "\n",
    "    @staticmethod\n",
    "    def df_loss(pred_dist, target):\n",
    "        # Distribution Focal Loss (DFL)\n",
    "        # https://ieeexplore.ieee.org/document/9792391\n",
    "        tl = target.long()  # target left\n",
    "        tr = tl + 1  # target right\n",
    "        wl = tr - target  # weight left\n",
    "        wr = 1 - wl  # weight right\n",
    "        left_loss = cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape)\n",
    "        right_loss = cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape)\n",
    "        return (left_loss * wl + right_loss * wr).mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50bcf6-f260-4c3e-9c86-f5bd2d1e8b99",
   "metadata": {},
   "source": [
    "## Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89ec03-e1a7-4e53-b601-e55abb58d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, model, params):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "        m = model.head  # Head() module\n",
    "\n",
    "        self.params = params\n",
    "        self.stride = m.stride\n",
    "        self.nc = m.nc\n",
    "        self.no = m.no\n",
    "        self.reg_max = m.ch\n",
    "        self.device = device\n",
    "\n",
    "        self.box_loss = BoxLoss(m.ch - 1).to(device)\n",
    "        self.cls_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.assigner = Assigner(nc=self.nc, top_k=10, alpha=0.5, beta=6.0)\n",
    "\n",
    "        self.project = torch.arange(m.ch, dtype=torch.float, device=device)\n",
    "\n",
    "    def box_decode(self, anchor_points, pred_dist):\n",
    "        b, a, c = pred_dist.shape\n",
    "        pred_dist = pred_dist.view(b, a, 4, c // 4)\n",
    "        pred_dist = pred_dist.softmax(3)\n",
    "        pred_dist = pred_dist.matmul(self.project.type(pred_dist.dtype))\n",
    "        lt, rb = pred_dist.chunk(2, -1)\n",
    "        x1y1 = anchor_points - lt\n",
    "        x2y2 = anchor_points + rb\n",
    "        return torch.cat(tensors=(x1y1, x2y2), dim=-1)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        x = torch.cat([i.view(outputs[0].shape[0], self.no, -1) for i in outputs], dim=2)\n",
    "        pred_distri, pred_scores = x.split(split_size=(self.reg_max * 4, self.nc), dim=1)\n",
    "\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        data_type = pred_scores.dtype\n",
    "        batch_size = pred_scores.shape[0]\n",
    "        input_size = torch.tensor(outputs[0].shape[2:], device=self.device, dtype=data_type) * self.stride[0]\n",
    "        anchor_points, stride_tensor = make_anchors(outputs, self.stride, offset=0.5)\n",
    "\n",
    "        idx = targets['idx'].view(-1, 1)\n",
    "        cls = targets['cls'].view(-1, 1)\n",
    "        box = targets['box']\n",
    "\n",
    "        targets = torch.cat((idx, cls, box), dim=1).to(self.device)\n",
    "        if targets.shape[0] == 0:\n",
    "            gt = torch.zeros(batch_size, 0, 5, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            gt = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n",
    "            for j in range(batch_size):\n",
    "                matches = i == j\n",
    "                n = matches.sum()\n",
    "                if n:\n",
    "                    gt[j, :n] = targets[matches, 1:]\n",
    "            x = gt[..., 1:5].mul_(input_size[[1, 0, 1, 0]])\n",
    "            y = torch.empty_like(x)\n",
    "            dw = x[..., 2] / 2  # half-width\n",
    "            dh = x[..., 3] / 2  # half-height\n",
    "            y[..., 0] = x[..., 0] - dw  # top left x\n",
    "            y[..., 1] = x[..., 1] - dh  # top left y\n",
    "            y[..., 2] = x[..., 0] + dw  # bottom right x\n",
    "            y[..., 3] = x[..., 1] + dh  # bottom right y\n",
    "            gt[..., 1:5] = y\n",
    "        gt_labels, gt_bboxes = gt.split((1, 4), 2)\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
    "\n",
    "        pred_bboxes = self.box_decode(anchor_points, pred_distri)\n",
    "        assigned_targets = self.assigner(pred_scores.detach().sigmoid(),\n",
    "                                         (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
    "                                         anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\n",
    "        target_bboxes, target_scores, fg_mask = assigned_targets\n",
    "\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        loss_cls = self.cls_loss(pred_scores, target_scores.to(data_type)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Box loss\n",
    "        loss_box = torch.zeros(1, device=self.device)\n",
    "        loss_dfl = torch.zeros(1, device=self.device)\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss_box, loss_dfl = self.box_loss(pred_distri,\n",
    "                                               pred_bboxes,\n",
    "                                               anchor_points,\n",
    "                                               target_bboxes,\n",
    "                                               target_scores,\n",
    "                                               target_scores_sum, fg_mask)\n",
    "\n",
    "        loss_box *= self.params['box']  # box gain\n",
    "        loss_cls *= self.params['cls']  # cls gain\n",
    "        loss_dfl *= self.params['dfl']  # dfl gain\n",
    "\n",
    "        return loss_box, loss_cls, loss_dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c5ee4-e14e-4c7d-abc8-68eacfbb98ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75af905-dbc7-4460-9e44-0aecbe0a4976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d11632-2e04-4f74-b2ce-dcdbb2636166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "PyTorch Version: 2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n",
    "print(\"PyTorch Version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a8a424-43fb-45b1-adab-9d57c6d169b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9943a59-db2e-4e6d-955a-0eb0b2542302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "\n",
    "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
    "\n",
    "voc_labels = ('UAV',)\n",
    "\n",
    "label_map = {k: v+1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "#Inverse mapping\n",
    "rev_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "#Colormap for bounding box\n",
    "CLASSES = 2\n",
    "distinct_colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                   for i in range(CLASSES)]\n",
    "label_color_map  = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
    "\n",
    "def save_label_map(output_path):\n",
    "    '''\n",
    "        Save label_map to output file JSON\n",
    "    '''\n",
    "    with open(os.path.join(output_path, \"label_map.json\"), \"w\") as j:\n",
    "        json.dump(label_map, j)\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    \n",
    "    for object in root.iter(\"object\"):\n",
    "        difficult = int(object.find(\"difficult\").text == \"1\")\n",
    "        label = object.find(\"name\").text.strip()\n",
    "        if label not in label_map:\n",
    "            print(\"{0} not in label map.\".format(label))\n",
    "            assert label in label_map\n",
    "            \n",
    "        bbox =  object.find(\"bndbox\")\n",
    "        xmin = int(bbox.find(\"xmin\").text)\n",
    "        ymin = int(bbox.find(\"ymin\").text)\n",
    "        xmax = int(bbox.find(\"xmax\").text)\n",
    "        ymax = int(bbox.find(\"ymax\").text)\n",
    "        \n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "        \n",
    "    return {\"boxes\": boxes, \"labels\": labels, \"difficulties\": difficulties}\n",
    "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
    "    '''\n",
    "        Resize image to (300, 300)  for SSD300\n",
    "        image: A PIL image\n",
    "        boxes: bounding boxes, a tensor of dimensions (n_objects, 4)\n",
    "        \n",
    "        Out:New image, new boxes or percent coordinates\n",
    "    '''\n",
    "    if type(image) != PIL.Image.Image:\n",
    "        image = TF.to_pil_image(image)\n",
    "    new_image= TF.resize(image, dims)\n",
    "\n",
    "    # Resize bounding boxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n",
    "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
    "    '''\n",
    "        Resize image to (300, 300)  for SSD300\n",
    "        image: A PIL image\n",
    "        boxes: bounding boxes, a tensor of dimensions (n_objects, 4)\n",
    "        \n",
    "        Out:New image, new boxes or percent coordinates\n",
    "    '''\n",
    "    if type(image) != PIL.Image.Image:\n",
    "        image = TF.to_pil_image(image)\n",
    "    new_image= TF.resize(image, dims)\n",
    "\n",
    "    # Resize bounding boxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes\n",
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    '''\n",
    "        Apply transformation\n",
    "        image: A PIL image\n",
    "        boxes: bounding boxe, a tensor of dimensions (n_objects, 4)\n",
    "        labels: labels of object a tensor of dimensions (n_object)\n",
    "        difficulties: difficulties of object detect, a tensor of dimensions (n_object)\n",
    "        split: one of \"TRAIN\", \"TEST\"\n",
    "        \n",
    "        Out: transformed images, transformed bounding boxes, transformed labels,\n",
    "        transformed difficulties\n",
    "    '''\n",
    "    \n",
    "    if type(image) != PIL.Image.Image:\n",
    "        image = TF.to_pil_image(image)\n",
    "    split = split.upper()\n",
    "    if split not in {\"TRAIN\", \"TEST\"}:\n",
    "        print(\"Param split in transform not in {TRAIN, TEST}\")\n",
    "        assert split in {\"TRAIN\", \"TEST\"}\n",
    "    \n",
    "    #mean and std from ImageNet\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "        \n",
    "    #Resize image to (300, 300)\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims= (300, 300))\n",
    "        \n",
    "    new_image = TF.to_tensor(new_image)\n",
    "    new_image = TF.normalize(new_image, mean=mean, std=std)\n",
    "    \n",
    "    return new_image, new_boxes, new_labels, new_difficulties\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "# from utils import transform\n",
    "class DUTDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform):\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = sorted(glob.glob(os.path.join(base_dir, 'img', '*.jpg')) +\n",
    "                             glob.glob(os.path.join(base_dir, 'img', '*.png')) +\n",
    "                             glob.glob(os.path.join(base_dir, 'img', '*.jpeg')))\n",
    "        \n",
    "        self.xmls = sorted(glob.glob(os.path.join(base_dir, 'xml', '*.xml')))\n",
    "        assert len(self.images) == len(self.xmls), \"Mismatch between image and annotation count\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.images[i]).convert(\"RGB\")\n",
    "        annotation = parse_annotation(self.xmls[i])\n",
    "\n",
    "        boxes = torch.FloatTensor(annotation[\"boxes\"])\n",
    "        labels = torch.LongTensor(annotation[\"labels\"])\n",
    "        difficulties = torch.ByteTensor(annotation[\"difficulties\"])\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes, labels, difficulties = self.transform(image, boxes, labels, difficulties,split='train')\n",
    "\n",
    "        return image, labels, boxes, difficulties\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        samples, cls, box, indices = zip(*batch)\n",
    "\n",
    "        cls = torch.cat(cls, dim=0)\n",
    "        box = torch.cat(box, dim=0)\n",
    "\n",
    "        new_indices = list(indices)\n",
    "        for i in range(len(indices)):\n",
    "            new_indices[i] += i\n",
    "        indices = torch.cat(new_indices, dim=0)\n",
    "\n",
    "        targets = {'cls': cls,\n",
    "                   'box': box,\n",
    "                   'idx': indices}\n",
    "        return torch.stack(samples, dim=0), targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = DUTDataset('train/train',transform)\n",
    "loader = data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4,\n",
    "                            pin_memory=True, collate_fn=DUTDataset.collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbd73b-816c-408f-bb9d-b46f402e00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "all_boxes = []\n",
    "all_labels = []\n",
    "all_difficulties = []\n",
    "\n",
    "num_batches = 1\n",
    "batch_count = 0\n",
    "\n",
    "all_images, all_boxes, all_labels, all_difficulties = [], [], [], []\n",
    "\n",
    "for images, boxes, labels, difficulties in loader:\n",
    "    all_images.extend(images)           # images: Tensor of shape [32, C, H, W]\n",
    "    all_boxes.extend(boxes)             # boxes: list of Tensors\n",
    "    all_labels.extend(labels)\n",
    "    all_difficulties.extend(difficulties)\n",
    "\n",
    "    batch_count += 1\n",
    "    if batch_count == num_batches:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489253b-4242-4ddb-a52d-4bb23f414698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad067c7a-b26d-4b64-8ff4-c1af3049b75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab1d53-4f46-4563-ae38-d2f276567817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
    "    # Convert nx4 boxes\n",
    "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xy2wh(x, w, h):\n",
    "    # warning: inplace clip\n",
    "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
    "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
    "\n",
    "    # Convert nx4 boxes\n",
    "    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def resample():\n",
    "    choices = (cv2.INTER_AREA,\n",
    "               cv2.INTER_CUBIC,\n",
    "               cv2.INTER_LINEAR,\n",
    "               cv2.INTER_NEAREST,\n",
    "               cv2.INTER_LANCZOS4)\n",
    "    return random.choice(seq=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c2dd5-19a8-4c74-bf06-2130c5449c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.util import make_anchors\n",
    "\n",
    "\n",
    "def fuse_conv(conv, norm):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 groups=conv.groups,\n",
    "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
    "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
    "\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
    "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
    "\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, k=1, s=1, p=0, g=1):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, p, groups=g, bias=False)\n",
    "        self.norm = torch.nn.BatchNorm2d(out_ch, eps=0.001, momentum=0.03)\n",
    "        self.relu = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.norm(self.conv(x)))\n",
    "\n",
    "    def fuse_forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, ch, e=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(ch, int(ch * e), torch.nn.SiLU(), k=3, p=1)\n",
    "        self.conv2 = Conv(int(ch * e), ch, torch.nn.SiLU(), k=3, p=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.conv1(x))\n",
    "\n",
    "\n",
    "class CSPModule(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, out_ch // 2, torch.nn.SiLU())\n",
    "        self.conv2 = Conv(in_ch, out_ch // 2, torch.nn.SiLU())\n",
    "        self.conv3 = Conv(2 * (out_ch // 2), out_ch, torch.nn.SiLU())\n",
    "        self.res_m = torch.nn.Sequential(Residual(out_ch // 2, e=1.0),\n",
    "                                         Residual(out_ch // 2, e=1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.res_m(self.conv1(x))\n",
    "        return self.conv3(torch.cat((y, self.conv2(x)), dim=1))\n",
    "\n",
    "\n",
    "class CSP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, n, csp, r):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, 2 * (out_ch // r), torch.nn.SiLU())\n",
    "        self.conv2 = Conv((2 + n) * (out_ch // r), out_ch, torch.nn.SiLU())\n",
    "\n",
    "        if not csp:\n",
    "            self.res_m = torch.nn.ModuleList(Residual(out_ch // r) for _ in range(n))\n",
    "        else:\n",
    "            self.res_m = torch.nn.ModuleList(CSPModule(out_ch // r, out_ch // r) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.conv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.res_m)\n",
    "        return self.conv2(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, in_ch // 2, torch.nn.SiLU())\n",
    "        self.conv2 = Conv(in_ch * 2, out_ch, torch.nn.SiLU())\n",
    "        self.res_m = torch.nn.MaxPool2d(k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.res_m(x)\n",
    "        y2 = self.res_m(y1)\n",
    "        return self.conv2(torch.cat(tensors=[x, y1, y2, self.res_m(y2)], dim=1))\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ch, num_head):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = ch // num_head\n",
    "        self.dim_key = self.dim_head // 2\n",
    "        self.scale = self.dim_key ** -0.5\n",
    "\n",
    "        self.qkv = Conv(ch, ch + self.dim_key * num_head * 2, torch.nn.Identity())\n",
    "\n",
    "        self.conv1 = Conv(ch, ch, torch.nn.Identity(), k=3, p=1, g=ch)\n",
    "        self.conv2 = Conv(ch, ch, torch.nn.Identity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(b, self.num_head, self.dim_key * 2 + self.dim_head, h * w)\n",
    "\n",
    "        q, k, v = qkv.split([self.dim_key, self.dim_key, self.dim_head], dim=2)\n",
    "\n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (v @ attn.transpose(-2, -1)).view(b, c, h, w) + self.conv1(v.reshape(b, c, h, w))\n",
    "        return self.conv2(x)\n",
    "\n",
    "\n",
    "class PSABlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ch, num_head):\n",
    "        super().__init__()\n",
    "        self.conv1 = Attention(ch, num_head)\n",
    "        self.conv2 = torch.nn.Sequential(Conv(ch, ch * 2, torch.nn.SiLU()),\n",
    "                                         Conv(ch * 2, ch, torch.nn.Identity()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.conv1(x)\n",
    "        return x + self.conv2(x)\n",
    "\n",
    "\n",
    "class PSA(torch.nn.Module):\n",
    "    def __init__(self, ch, n):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(ch, 2 * (ch // 2), torch.nn.SiLU())\n",
    "        self.conv2 = Conv(2 * (ch // 2), ch, torch.nn.SiLU())\n",
    "        self.res_m = torch.nn.Sequential(*(PSABlock(ch // 2, ch // 128) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, y = self.conv1(x).chunk(2, 1)\n",
    "        return self.conv2(torch.cat(tensors=(x, self.res_m(y)), dim=1))\n",
    "\n",
    "\n",
    "class DarkNet(torch.nn.Module):\n",
    "    def __init__(self, width, depth, csp):\n",
    "        super().__init__()\n",
    "        self.p1 = []\n",
    "        self.p2 = []\n",
    "        self.p3 = []\n",
    "        self.p4 = []\n",
    "        self.p5 = []\n",
    "\n",
    "        # p1/2\n",
    "        self.p1.append(Conv(width[0], width[1], torch.nn.SiLU(), k=3, s=2, p=1))\n",
    "        # p2/4\n",
    "        self.p2.append(Conv(width[1], width[2], torch.nn.SiLU(), k=3, s=2, p=1))\n",
    "        self.p2.append(CSP(width[2], width[3], depth[0], csp[0], r=4))\n",
    "        # p3/8\n",
    "        self.p3.append(Conv(width[3], width[3], torch.nn.SiLU(), k=3, s=2, p=1))\n",
    "        self.p3.append(CSP(width[3], width[4], depth[1], csp[0], r=4))\n",
    "        # p4/16\n",
    "        self.p4.append(Conv(width[4], width[4], torch.nn.SiLU(), k=3, s=2, p=1))\n",
    "        self.p4.append(CSP(width[4], width[4], depth[2], csp[1], r=2))\n",
    "        # p5/32\n",
    "        self.p5.append(Conv(width[4], width[5], torch.nn.SiLU(), k=3, s=2, p=1))\n",
    "        self.p5.append(CSP(width[5], width[5], depth[3], csp[1], r=2))\n",
    "        self.p5.append(SPP(width[5], width[5]))\n",
    "        self.p5.append(PSA(width[5], depth[4]))\n",
    "\n",
    "        self.p1 = torch.nn.Sequential(*self.p1)\n",
    "        self.p2 = torch.nn.Sequential(*self.p2)\n",
    "        self.p3 = torch.nn.Sequential(*self.p3)\n",
    "        self.p4 = torch.nn.Sequential(*self.p4)\n",
    "        self.p5 = torch.nn.Sequential(*self.p5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.p2(p1)\n",
    "        p3 = self.p3(p2)\n",
    "        p4 = self.p4(p3)\n",
    "        p5 = self.p5(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DarkFPN(torch.nn.Module):\n",
    "    def __init__(self, width, depth, csp):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.Upsample(scale_factor=2)\n",
    "        self.h1 = CSP(width[4] + width[5], width[4], depth[5], csp[0], r=2)\n",
    "        self.h2 = CSP(width[4] + width[4], width[3], depth[5], csp[0], r=2)\n",
    "        self.h3 = Conv(width[3], width[3], torch.nn.SiLU(), k=3, s=2, p=1)\n",
    "        self.h4 = CSP(width[3] + width[4], width[4], depth[5], csp[0], r=2)\n",
    "        self.h5 = Conv(width[4], width[4], torch.nn.SiLU(), k=3, s=2, p=1)\n",
    "        self.h6 = CSP(width[4] + width[5], width[5], depth[5], csp[1], r=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = x\n",
    "        p4 = self.h1(torch.cat(tensors=[self.up(p5), p4], dim=1))\n",
    "        p3 = self.h2(torch.cat(tensors=[self.up(p4), p3], dim=1))\n",
    "        p4 = self.h4(torch.cat(tensors=[self.h3(p3), p4], dim=1))\n",
    "        p5 = self.h6(torch.cat(tensors=[self.h5(p4), p5], dim=1))\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DFL(torch.nn.Module):\n",
    "    # Generalized Focal Loss\n",
    "    # https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = torch.nn.Conv2d(ch, out_channels=1, kernel_size=1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
    "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape\n",
    "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
    "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
    "\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    anchors = torch.empty(0)\n",
    "    strides = torch.empty(0)\n",
    "\n",
    "    def __init__(self, nc=80, filters=()):\n",
    "        super().__init__()\n",
    "        self.ch = 16  # DFL channels\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(filters)  # number of detection layers\n",
    "        self.no = nc + self.ch * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "\n",
    "        box = max(64, filters[0] // 4)\n",
    "        cls = max(80, filters[0], self.nc)\n",
    "\n",
    "        self.dfl = DFL(self.ch)\n",
    "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, box,torch.nn.SiLU(), k=3, p=1),\n",
    "                                                           Conv(box, box,torch.nn.SiLU(), k=3, p=1),\n",
    "                                                           torch.nn.Conv2d(box, out_channels=4 * self.ch,\n",
    "                                                                           kernel_size=1)) for x in filters)\n",
    "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, x, torch.nn.SiLU(), k=3, p=1, g=x),\n",
    "                                                           Conv(x, cls, torch.nn.SiLU()),\n",
    "                                                           Conv(cls, cls, torch.nn.SiLU(), k=3, p=1, g=cls),\n",
    "                                                           Conv(cls, cls, torch.nn.SiLU()),\n",
    "                                                           torch.nn.Conv2d(cls, out_channels=self.nc,\n",
    "                                                                           kernel_size=1)) for x in filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (box, cls) in enumerate(zip(self.box, self.cls)):\n",
    "            x[i] = torch.cat(tensors=(box(x[i]), cls(x[i])), dim=1)\n",
    "        if self.training:\n",
    "            return x\n",
    "\n",
    "        self.anchors, self.strides = (i.transpose(0, 1) for i in make_anchors(x, self.stride))\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2)\n",
    "        box, cls = x.split(split_size=(4 * self.ch, self.nc), dim=1)\n",
    "\n",
    "        a, b = self.dfl(box).chunk(2, 1)\n",
    "        a = self.anchors.unsqueeze(0) - a\n",
    "        b = self.anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n",
    "\n",
    "        return torch.cat(tensors=(box * self.strides, cls.sigmoid()), dim=1)\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        # Initialize biases\n",
    "        # WARNING: requires stride availability\n",
    "        for box, cls, s in zip(self.box, self.cls, self.stride):\n",
    "            # box\n",
    "            box[-1].bias.data[:] = 1.0\n",
    "            # cls (.01 objects, 80 classes, 640 image)\n",
    "            cls[-1].bias.data[:self.nc] = math.log(5 / self.nc / (640 / s) ** 2)\n",
    "\n",
    "\n",
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, width, depth, csp, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth, csp)\n",
    "        self.fpn = DarkFPN(width, depth, csp)\n",
    "\n",
    "        img_dummy = torch.zeros(1, width[0], 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fpn(x)\n",
    "        return self.head(list(x))\n",
    "\n",
    "    def fuse(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self\n",
    "\n",
    "\n",
    "def yolo_v11_n(num_classes: int = 80):\n",
    "    csp = [False, True]\n",
    "    depth = [1, 1, 1, 1, 1, 1]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO(width, depth, csp, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v11_t(num_classes: int = 80):\n",
    "    csp = [False, True]\n",
    "    depth = [1, 1, 1, 1, 1, 1]\n",
    "    width = [3, 24, 48, 96, 192, 384]\n",
    "    return YOLO(width, depth, csp, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v11_s(num_classes: int = 80):\n",
    "    csp = [False, True]\n",
    "    depth = [1, 1, 1, 1, 1, 1]\n",
    "    width = [3, 32, 64, 128, 256, 512]\n",
    "    return YOLO(width, depth, csp, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v11_m(num_classes: int = 80):\n",
    "    csp = [True, True]\n",
    "    depth = [1, 1, 1, 1, 1, 1]\n",
    "    width = [3, 64, 128, 256, 512, 512]\n",
    "    return YOLO(width, depth, csp, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v11_l(num_classes: int = 80):\n",
    "    csp = [True, True]\n",
    "    depth = [2, 2, 2, 2, 2, 2]\n",
    "    width = [3, 64, 128, 256, 512, 512]\n",
    "    return YOLO(width, depth, csp, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v11_x(num_classes: int = 80):\n",
    "    csp = [True, True]\n",
    "    depth = [2, 2, 2, 2, 2, 2]\n",
    "    width = [3, 96, 192, 384, 768, 768]\n",
    "    return YOLO(width, depth, csp, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddcbf118-f5a3-44ef-ba92-3d58f9e34f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Training started\n",
      "Epoch: [0][0/162]\tLoss 16.1204 ( Average Loss per epoch: 18.0112)\n",
      "Epoch: [0][100/162]\tLoss 5.6982 ( Average Loss per epoch: 10.1025)\n",
      "Epoch: [1][0/162]\tLoss 6.8103 ( Average Loss per epoch: 8.2227)\n",
      "Epoch: [1][100/162]\tLoss 4.8821 ( Average Loss per epoch: 6.0014)\n",
      "Epoch: [2][0/162]\tLoss 3.4762 ( Average Loss per epoch: 3.4762)\n",
      "Epoch: [2][100/162]\tLoss 3.0842 ( Average Loss per epoch: 4.0256)\n",
      "Epoch: [3][0/162]\tLoss 3.2156 ( Average Loss per epoch: 3.2156)\n",
      "Epoch: [3][100/162]\tLoss 2.7519 ( Average Loss per epoch: 3.2895)\n",
      "Epoch: [4][0/162]\tLoss 2.7208 ( Average Loss per epoch: 2.7208)\n",
      "Epoch: [4][100/162]\tLoss 2.4793 ( Average Loss per epoch: 2.5662)\n",
      "Epoch: [5][0/162]\tLoss 2.2584 ( Average Loss per epoch: 2.2584)\n",
      "Epoch: [5][100/162]\tLoss 2.9331 ( Average Loss per epoch: 2.3218)\n",
      "Epoch: [6][0/162]\tLoss 2.0065 ( Average Loss per epoch: 2.0065)\n",
      "Epoch: [6][100/162]\tLoss 1.8974 ( Average Loss per epoch: 1.9742)\n",
      "Epoch: [7][0/162]\tLoss 1.7986 ( Average Loss per epoch: 1.7986)\n",
      "Epoch: [7][100/162]\tLoss 1.4132 ( Average Loss per epoch: 1.7592)\n",
      "Epoch: [8][0/162]\tLoss 1.3825 ( Average Loss per epoch: 1.3825)\n",
      "Epoch: [8][100/162]\tLoss 1.7421 ( Average Loss per epoch: 1.6859)\n",
      "Epoch: [9][0/162]\tLoss 1.9120 ( Average Loss per epoch: 1.9120)\n",
      "Epoch: [9][100/162]\tLoss 1.2642 ( Average Loss per epoch: 1.5934)\n",
      "Epoch: [10][0/162]\tLoss 1.2671 ( Average Loss per epoch: 1.2671)\n",
      "Epoch: [10][100/162]\tLoss 1.2158 ( Average Loss per epoch: 1.3567)\n",
      "Epoch: [11][0/162]\tLoss 1.2214 ( Average Loss per epoch: 1.2214)\n",
      "Epoch: [11][100/162]\tLoss 1.1184 ( Average Loss per epoch: 1.3551)\n",
      "Epoch: [12][0/162]\tLoss 1.0127 ( Average Loss per epoch: 1.0127)\n",
      "Epoch: [12][100/162]\tLoss 1.1847 ( Average Loss per epoch: 1.1139)\n",
      "Epoch: [13][0/162]\tLoss 0.7652 ( Average Loss per epoch: 0.7652)\n",
      "Epoch: [13][100/162]\tLoss 0.9351 ( Average Loss per epoch: 0.9795)\n",
      "Epoch: [14][0/162]\tLoss 0.9335 ( Average Loss per epoch: 0.9335)\n",
      "Epoch: [14][100/162]\tLoss 1.3861 ( Average Loss per epoch: 0.9107)\n",
      "Epoch: [15][0/162]\tLoss 0.6674 ( Average Loss per epoch: 0.6674)\n",
      "Epoch: [15][100/162]\tLoss 1.0289 ( Average Loss per epoch: 0.8321)\n",
      "Epoch: [16][0/162]\tLoss 0.5399 ( Average Loss per epoch: 0.5399)\n",
      "Epoch: [16][100/162]\tLoss 0.6124 ( Average Loss per epoch: 0.7567)\n",
      "Epoch: [17][0/162]\tLoss 0.3873 ( Average Loss per epoch: 0.3873)\n",
      "Epoch: [17][100/162]\tLoss 0.7386 ( Average Loss per epoch: 0.7941)\n",
      "Epoch: [18][0/162]\tLoss 0.6522 ( Average Loss per epoch: 0.6522)\n",
      "Epoch: [18][100/162]\tLoss 0.5034 ( Average Loss per epoch: 0.6131)\n",
      "Epoch: [19][0/162]\tLoss 0.4770 ( Average Loss per epoch: 0.4770)\n",
      "Epoch: [19][100/162]\tLoss 0.6395 ( Average Loss per epoch: 0.6302)\n",
      "Epoch: [20][0/162]\tLoss 0.4331 ( Average Loss per epoch: 0.4331)\n",
      "Epoch: [20][100/162]\tLoss 0.4548 ( Average Loss per epoch: 0.5396)\n",
      "Epoch: [21][0/162]\tLoss 0.7125 ( Average Loss per epoch: 0.7125)\n",
      "Epoch: [21][100/162]\tLoss 0.5231 ( Average Loss per epoch: 0.4958)\n",
      "Epoch: [22][0/162]\tLoss 0.4310 ( Average Loss per epoch: 0.4310)\n",
      "Epoch: [22][100/162]\tLoss 0.3917 ( Average Loss per epoch: 0.4482)\n",
      "Epoch: [23][0/162]\tLoss 0.1897 ( Average Loss per epoch: 0.1897)\n",
      "Epoch: [23][100/162]\tLoss 0.3420 ( Average Loss per epoch: 0.4091)\n",
      "Epoch: [24][0/162]\tLoss 0.3978 ( Average Loss per epoch: 0.3978)\n",
      "Epoch: [24][100/162]\tLoss 0.3911 ( Average Loss per epoch: 0.3704)\n",
      "Epoch: [25][0/162]\tLoss 0.3112 ( Average Loss per epoch: 0.3112)\n",
      "Epoch: [25][100/162]\tLoss 0.3596 ( Average Loss per epoch: 0.3528)\n",
      "Epoch: [26][0/162]\tLoss 0.3440 ( Average Loss per epoch: 0.3440)\n",
      "Epoch: [26][100/162]\tLoss 0.2457 ( Average Loss per epoch: 0.2961)\n",
      "Epoch: [27][0/162]\tLoss 0.2512 ( Average Loss per epoch: 0.2512)\n",
      "Epoch: [27][100/162]\tLoss 0.3124 ( Average Loss per epoch: 0.3080)\n",
      "Epoch: [28][0/162]\tLoss 0.2003 ( Average Loss per epoch: 0.2003)\n",
      "Epoch: [28][100/162]\tLoss 0.1816 ( Average Loss per epoch: 0.2827)\n",
      "Epoch: [29][0/162]\tLoss 0.2622 ( Average Loss per epoch: 0.2622)\n",
      "Epoch: [29][100/162]\tLoss 0.1714 ( Average Loss per epoch: 0.2519)\n",
      "Epoch: [30][0/162]\tLoss 0.1692 ( Average Loss per epoch: 0.1692)\n",
      "Epoch: [30][100/162]\tLoss 0.1835 ( Average Loss per epoch: 0.1938)\n",
      "Epoch: [31][0/162]\tLoss 0.1804 ( Average Loss per epoch: 0.1804)\n",
      "Epoch: [31][100/162]\tLoss 0.1740 ( Average Loss per epoch: 0.1639)\n",
      "Epoch: [32][0/162]\tLoss 0.0864 ( Average Loss per epoch: 0.0864)\n",
      "Epoch: [32][100/162]\tLoss 0.1982 ( Average Loss per epoch: 0.1583)\n",
      "Epoch: [33][0/162]\tLoss 0.2335 ( Average Loss per epoch: 0.2335)\n",
      "Epoch: [33][100/162]\tLoss 0.1325 ( Average Loss per epoch: 0.1653)\n",
      "Epoch: [34][0/162]\tLoss 0.1879 ( Average Loss per epoch: 0.1879)\n",
      "Epoch: [34][100/162]\tLoss 0.0841 ( Average Loss per epoch: 0.1145)\n",
      "Epoch: [35][0/162]\tLoss 0.1736 ( Average Loss per epoch: 0.1736)\n",
      "Epoch: [35][100/162]\tLoss 0.1374 ( Average Loss per epoch: 0.1545)\n",
      "Epoch: [36][0/162]\tLoss 0.1835 ( Average Loss per epoch: 0.1835)\n",
      "Epoch: [36][100/162]\tLoss 0.1412 ( Average Loss per epoch: 0.1629)\n",
      "Epoch: [37][0/162]\tLoss 0.1695 ( Average Loss per epoch: 0.1695)\n",
      "Epoch: [37][100/162]\tLoss 0.1501 ( Average Loss per epoch: 0.1647)\n",
      "Epoch: [38][0/162]\tLoss 0.1540 ( Average Loss per epoch: 0.1540)\n",
      "Epoch: [38][100/162]\tLoss 0.1689 ( Average Loss per epoch: 0.1556)\n",
      "Epoch: [39][0/162]\tLoss 0.1423 ( Average Loss per epoch: 0.1423)\n",
      "Epoch: [39][100/162]\tLoss 0.1560 ( Average Loss per epoch: 0.1491)\n",
      "Epoch: [40][0/162]\tLoss 0.1459 ( Average Loss per epoch: 0.1459)\n",
      "Epoch: [40][100/162]\tLoss 0.1420 ( Average Loss per epoch: 0.1438)\n",
      "Epoch: [41][0/162]\tLoss 0.1565 ( Average Loss per epoch: 0.1565)\n",
      "Epoch: [41][100/162]\tLoss 0.1256 ( Average Loss per epoch: 0.1457)\n",
      "Epoch: [42][0/162]\tLoss 0.1602 ( Average Loss per epoch: 0.1602)\n",
      "Epoch: [42][100/162]\tLoss 0.1347 ( Average Loss per epoch: 0.1539)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "\n",
    "from nets import nn\n",
    "from utils import util\n",
    "from utils.dataset import Dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_dir = '../Dataset/COCO'\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    '''\n",
    "        One epoch's training\n",
    "    '''\n",
    "    model.train()\n",
    "    losses = Metrics()\n",
    "    \n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "        \n",
    "        #Foward pass\n",
    "        locs_pred, cls_pred = model(images)\n",
    "        \n",
    "        #loss\n",
    "        loss = criterion(locs_pred, cls_pred, boxes, labels)\n",
    "        \n",
    "        #Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if grad_clip is not None:\n",
    "            clip_grad(optimizer, grad_clip)\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t' 'Loss {loss.val:.4f} ( Average Loss per epoch: {loss.avg:.4f})\\t'.format(epoch, i, len(train_loader), loss=losses))\n",
    "    del locs_pred, cls_pred, images, boxes, labels\n",
    "\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(save, f='./weights/last.pt')\n",
    "                if best == last[0]:\n",
    "                    torch.save(save, f='./weights/best.pt')\n",
    "                del save\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        util.strip_optimizer('./weights/best.pt')  # strip optimizers\n",
    "        util.strip_optimizer('./weights/last.pt')  # strip optimizers\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(args, params, model=None):\n",
    "    filenames = []\n",
    "    with open(f'{data_dir}/val2017.txt') as f:\n",
    "        for filename in f.readlines():\n",
    "            filename = os.path.basename(filename.rstrip())\n",
    "            filenames.append(f'{data_dir}/images/val2017/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, args.input_size, params, augment=False)\n",
    "    loader = data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4,\n",
    "                             pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    plot = False\n",
    "    if not model:\n",
    "        plot = True\n",
    "        model = torch.load(f='./weights/best.pt', map_location='cuda')\n",
    "        model = model['model'].float().fuse()\n",
    "\n",
    "    model.half()\n",
    "    model.eval()\n",
    "\n",
    "    # Configure\n",
    "    iou_v = torch.linspace(start=0.5, end=0.95, steps=10).cuda()  # iou vector for mAP@0.5:0.95\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    m_pre = 0\n",
    "    m_rec = 0\n",
    "    map50 = 0\n",
    "    mean_ap = 0\n",
    "    metrics = []\n",
    "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 5) % ('', 'precision', 'recall', 'mAP50', 'mAP'))\n",
    "    for samples, targets in p_bar:\n",
    "        samples = samples.cuda()\n",
    "        samples = samples.half()  # uint8 to fp16/32\n",
    "        samples = samples / 255.  # 0 - 255 to 0.0 - 1.0\n",
    "        _, _, h, w = samples.shape  # batch-size, channels, height, width\n",
    "        scale = torch.tensor((w, h, w, h)).cuda()\n",
    "        # Inference\n",
    "        outputs = model(samples)\n",
    "        # NMS\n",
    "        outputs = util.non_max_suppression(outputs)\n",
    "        # Metrics\n",
    "        for i, output in enumerate(outputs):\n",
    "            idx = targets['idx'] == i\n",
    "            cls = targets['cls'][idx]\n",
    "            box = targets['box'][idx]\n",
    "\n",
    "            cls = cls.cuda()\n",
    "            box = box.cuda()\n",
    "\n",
    "            metric = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "\n",
    "            if output.shape[0] == 0:\n",
    "                if cls.shape[0]:\n",
    "                    metrics.append((metric, *torch.zeros((2, 0)).cuda(), cls.squeeze(-1)))\n",
    "                continue\n",
    "            # Evaluate\n",
    "            if cls.shape[0]:\n",
    "                target = torch.cat(tensors=(cls, util.wh2xy(box) * scale), dim=1)\n",
    "                metric = util.compute_metric(output[:, :6], target, iou_v)\n",
    "            # Append\n",
    "            metrics.append((metric, output[:, 4], output[:, 5], cls.squeeze(-1)))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = [torch.cat(x, dim=0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*metrics, plot=plot, names=params[\"names\"])\n",
    "    # Print results\n",
    "    print(('%10s' + '%10.3g' * 4) % ('', m_pre, m_rec, map50, mean_ap))\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    return mean_ap, map50, m_rec, m_pre\n",
    "\n",
    "\n",
    "def profile(args, params):\n",
    "    import thop\n",
    "    shape = (1, 3, args.input_size, args.input_size)\n",
    "    model = nn.yolo_v11_n(len(params['names'])).fuse()\n",
    "\n",
    "    model.eval()\n",
    "    model(torch.zeros(shape))\n",
    "\n",
    "    x = torch.empty(shape)\n",
    "    flops, num_params = thop.profile(model, inputs=[x], verbose=False)\n",
    "    flops, num_params = thop.clever_format(nums=[2 * flops, num_params], format=\"%.3f\")\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Number of parameters: {num_params}')\n",
    "        print(f'Number of FLOPs: {flops}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--input-size', default=640, type=int)\n",
    "    parser.add_argument('--batch-size', default=32, type=int)\n",
    "    parser.add_argument('--local-rank', default=0, type=int)\n",
    "    parser.add_argument('--local_rank', default=0, type=int)\n",
    "    parser.add_argument('--epochs', default=600, type=int)\n",
    "    parser.add_argument('--train', action='store_true')\n",
    "    parser.add_argument('--test', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.local_rank = int(os.getenv('LOCAL_RANK', 0))\n",
    "    args.world_size = int(os.getenv('WORLD_SIZE', 1))\n",
    "    args.distributed = int(os.getenv('WORLD_SIZE', 1)) > 1\n",
    "\n",
    "    if args.distributed:\n",
    "        torch.cuda.set_device(device=args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        if not os.path.exists('weights'):\n",
    "            os.makedirs('weights')\n",
    "\n",
    "    with open('utils/args.yaml', errors='ignore') as f:\n",
    "        params = yaml.safe_load(f)\n",
    "\n",
    "    util.setup_seed()\n",
    "    util.setup_multi_processes()\n",
    "\n",
    "    profile(args, params)\n",
    "\n",
    "    if args.train:\n",
    "         train(train_loader = train_loader, model = model, criterion= criterion,\n",
    "              optimizer = optimizer, epoch = epoch)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a78c2-42fb-4a33-9851-345ae5f7a728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
